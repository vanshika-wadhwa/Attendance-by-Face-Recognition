{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4afcc3b8-5f2d-474e-85d0-c6a78372eb75",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     10\u001b[0m     success,img\u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()    \u001b[38;5;66;03m# once image read \u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mface identityfy \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m)    \u001b[38;5;66;03m#1-->milli second \u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# boiler plate for running web cam\n",
    "cap=cv2.VideoCapture(1)  #1 for multiple camera\n",
    "cap.set(3,1280)   #width should be 1280-------->graphics\n",
    "cap.set(4,720)  #height 720---------->graphics\n",
    "\n",
    "\n",
    "while True:\n",
    "    success,img= cap.read()    # once image read \n",
    "    cv2.imshow(\"face identityfy \",img)\n",
    "    cv2.waitKey(1)    #1-->milli second "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a5d5ed25-a96f-46f4-ad96-169cb194db7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3586364332.py, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[22], line 19\u001b[1;36m\u001b[0m\n\u001b[1;33m    import opencv-python\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import sys\n",
    "import click\n",
    "import itertools \n",
    "import multiprocessing as mp\n",
    "import print_function\n",
    "import dlib\n",
    "import face_recognition_models\n",
    "import setuptools\n",
    "import api\n",
    "import pytest\n",
    "import unittest\n",
    "import PIL\n",
    "import Pillow\n",
    "import mock\n",
    "import neighbors\n",
    "import distance\n",
    "import scipy.spatial\n",
    "import opencv-python\n",
    "import cv2\n",
    "import MagicMock\n",
    "import scikit-learn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pickle\n",
    "import math\n",
    "import threading\n",
    "import platform\n",
    "import picamera\n",
    "import deepface\n",
    "import base\n",
    "import SQLAlchemy\n",
    "import Mail, Message\n",
    "import secure_filename\n",
    "import IntegrityError\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dae421-705f-4153-8acd-e2febb40bd1e",
   "metadata": {},
   "source": [
    "# webcamera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e1cbaaac-559d-46ac-8ec2-99cf4b5857eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't read frame\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "# Boilerplate for running webcam\n",
    "cap = cv2.VideoCapture(0)  # Change camera index as needed   0-->default camera / 1-->for multicamera\n",
    "if not cap.isOpened():\n",
    "    print(\"Couldn't open camera\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "cap.set(3, 1280)  # Width=1280  --->graphic\n",
    "cap.set(4, 720)   # Height=720  --->graphic\n",
    "\n",
    "\n",
    "imgBackground=cv2.imread('resources/background.png')       #background store h resources folder m\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()  # Read frame\n",
    "    if not success:\n",
    "        print(\"Couldn't read frame\")\n",
    "        break\n",
    "    \n",
    "    cv2.imshow(\"webcam\", img)\n",
    "    cv2.imshow(\"face identification\",imgBackground)       #actual output\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b8b892-23c8-4811-a950-a50b152c014b",
   "metadata": {},
   "source": [
    "# graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b1b520f-7637-4cd8-9ff6-2582bc5994fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 29\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     success, img \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()  \u001b[38;5;66;03m# Read frame\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     \u001b[43mimgBackground\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m480\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m img  \u001b[38;5;66;03m# Adjust dimensions to match the frame from the webcam\u001b[39;00m\n\u001b[0;32m     30\u001b[0m                                             \u001b[38;5;66;03m#9*3 strting point from top left corner\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     folderModePath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mface recogination real db\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mresources\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmodes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'NoneType'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "# Boilerplate for running webcam\n",
    "cap = cv2.VideoCapture(0)  # Change camera index as needed   0-->default camera / 1-->for multicamera\n",
    "if not cap.isOpened():\n",
    "    print(\"Couldn't open camera\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "cap.set(3, 640)  # Width=640  --->graphic\n",
    "cap.set(4, 480)   # Height=480  --->graphic\n",
    "\n",
    "\n",
    "imgBackground=cv2.imread(r'C:\\Users\\user\\Desktop\\face recogination real db\\resources\\background.png')       # location of background.png ---> where opencv request to get details\n",
    "\n",
    "if imgBackground is None:\n",
    "    print(\"Couldn't load background image\")\n",
    "    exit()\n",
    "     #importing the mode images into a list\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "while True:\n",
    "    \n",
    "    success, img = cap.read()  # Read frame\n",
    "\n",
    "    \n",
    "    imgBackground[9:9+480, 3:3+640] = img  # Adjust dimensions to match the frame from the webcam\n",
    "                                            #9*3 strting point from top left corner\n",
    "    folderModePath=r\"C:\\Users\\user\\Desktop\\face recogination real db\\resources\\modes\"\n",
    "    ModePath=os.listdir(folderModePath)\n",
    "    imgModeList=[]\n",
    "    \n",
    "    print(\"List of files in the directory:\")\n",
    "    for path in ModePath:\n",
    "        imgModeList.append(cv2.imread(os.path.join(folderModePath,path)))     \n",
    "        img_mode_resized = cv2.resize(imgModeList, (640, 480))  # Resize image to match region dimensions\n",
    "        imgModeList.append(img_mode_resized) \n",
    "        \n",
    "    print(\"count of file is:\",len(imgModeList))  #count of loaded images\n",
    "\n",
    "    \n",
    "    imgBackground[44:44+633,808:808+414] = imgModeList[0]  #--> 0=active\n",
    "    \n",
    "   \n",
    "    if not success:\n",
    "        print(\"Couldn't read frame\")\n",
    "        break\n",
    "    \n",
    "    cv2.imshow(\"webcam\", img)\n",
    "    cv2.imshow(\"face identification\",imgBackground)       #actual output\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "10dd6793-37a7-4f22-ba64-04bad221ccbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded images: 4\n",
      "Couldn't read frame\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # Change camera index as needed\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Couldn't open camera\")\n",
    "    exit()\n",
    "\n",
    "cap.set(3, 640)  # Width=640\n",
    "cap.set(4, 480)  # Height=480\n",
    "\n",
    "imgBackground = cv2.imread(r'C:\\Users\\user\\Desktop\\face recogination real db\\resources\\background.png')\n",
    "\n",
    "if imgBackground is None:\n",
    "    print(\"Couldn't load background image\")\n",
    "    exit()\n",
    "\n",
    "folderModePath = r\"C:\\Users\\user\\Desktop\\face recogination real db\\resources\\modes\"\n",
    "ModePath = os.listdir(folderModePath)\n",
    "imgModeList = []\n",
    "\n",
    "for path in ModePath:\n",
    "    img_mode = cv2.imread(os.path.join(folderModePath, path))\n",
    "    if img_mode is None:\n",
    "        print(f\"Couldn't load image: {path}\")\n",
    "        continue\n",
    "    img_mode_resized = cv2.resize(img_mode, (640, 480))\n",
    "    imgModeList.append(img_mode_resized)\n",
    "\n",
    "print(\"Number of loaded images:\", len(imgModeList))\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        print(\"Couldn't read frame\")\n",
    "        break\n",
    "\n",
    "    imgBackground[9:9+480, 3:3+640] = img\n",
    "    #imgBackground[44:44+633,808:808+414]=imgModeList[1]        #marked,active mode se link krega\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    cv2.imshow(\"webcam\", img)\n",
    "    cv2.imshow(\"face identification\", imgBackground)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36721b19-a25a-4ea3-92b9-337882b667f9",
   "metadata": {},
   "source": [
    "# encding generator: generate data or encode data for all measurements and store it into files  and that file is importing for face recoginition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7cd322ad-b9ed-461e-943c-ba794a401563",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 60 (3276513760.py, line 61)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[31], line 61\u001b[1;36m\u001b[0m\n\u001b[1;33m    matches = face_recognition.compare_faces(encodeListKnown, encodeFace)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'for' statement on line 60\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # Change camera index as needed\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Couldn't open camera\")\n",
    "    exit()\n",
    "\n",
    "cap.set(3, 640)  # Width=640\n",
    "cap.set(4, 480)  # Height=480\n",
    "\n",
    "imgBackground = cv2.imread(r'C:\\Users\\user\\Desktop\\face recogination real db\\resources\\background.png')\n",
    "\n",
    "if imgBackground is None:\n",
    "    print(\"Couldn't load background image\")\n",
    "    exit()\n",
    "\n",
    "folderModePath = r\"C:\\Users\\user\\Desktop\\face recogination real db\\resources\\modes\"\n",
    "ModePath = os.listdir(folderModePath)\n",
    "imgModeList = []\n",
    "\n",
    "for path in ModePath:\n",
    "    img_mode = cv2.imread(os.path.join(folderModePath, path))\n",
    "    if img_mode is None:\n",
    "        print(f\"Couldn't load image: {path}\")\n",
    "        continue\n",
    "    img_mode_resized = cv2.resize(img_mode, (640, 480))\n",
    "    imgModeList.append(img_mode_resized)\n",
    "\n",
    "print(\"Number of loaded images:\", len(imgModeList))\n",
    "\n",
    "print(\"loading the encoding file\")\n",
    "file=open('encodeFile.p','rb')\n",
    "encodeListKnownWithIds = pickle.load(file)   # load hui file now extract\n",
    "file.close()\n",
    "encodeListKnown,studentId = encodeListKnownWithIds   #encode generator m dono ko ikthe kiya tha yha alg  for extraction\n",
    "print(\"the encoding file loaded\")\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        print(\"Couldn't read frame\")\n",
    "        break\n",
    "\n",
    "    imgSmall=cv2.resize(img,(0,0),None,0.25,0.25) # (0,0)-->pixel size 0.25,0.25 --> koi b size ho img ka use 1/4th krdo\n",
    "    imgSmall=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faceCurrFrame=face_recognition.face_locations(imgSmall)\n",
    "    encodeCurrFrame=face_recognition.face_encodings(imgSmall,faceCurrFrame)  # sare faces ki encoding ni krni curently available face ki krni h -->given faceCurrFrame\n",
    "    imgBackground[9:9+480, 3:3+640] = img\n",
    "\n",
    "    # Resize the image from imgModeList[1] to match the dimensions\n",
    "    img_mode_resized = cv2.resize(imgModeList[1], (414, 633))\n",
    "    imgBackground[44:44+633,808:808+414] = img_mode_resized\n",
    "\n",
    "    for encodeFace, faceLoc in zip(encodeCurrFrame, faceCurrFrame):\n",
    "    matches = face_recognition.compare_faces(encodeListKnown, encodeFace)\n",
    "    faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)\n",
    "\n",
    "    print(\"matches\", matches)\n",
    "    print(\"face distance\", faceDis)\n",
    "\n",
    "    matchIndex = np.argmin(faceDis)  # decimals ki jagha integers se frequesncy pta chlegi\n",
    "\n",
    "    if matches[matchIndex]:\n",
    "        print(\"known face detected\")\n",
    "'''\n",
    "    for encodeFace, faceLoc in zip(encodeCurrFrame,faceCurrFrame):         #dono arguments k liye loop chaihe at same time -->by zip dono arguments ki info bhar k corressponding var m store hogi\n",
    "        matches=face_recognition.compare_faces(encodeListKnown,encodeFace)            #current face-->encodeFace mathces m compared face store hoga from list of known faces\n",
    "        faceDis=face_recognition.facedistance(encodeListKnown,encodeFace)        #\n",
    "\n",
    "        print(\"matches\",matches)\n",
    "        print(\"face distance\",faceDis)\n",
    "    \n",
    "         matchIndex=np.argmin(faceDis)       # decimals ki jagha integers se frequesncy pta chlegi\n",
    "\n",
    "'''\n",
    "    # frequency of matching faces (0.8,0.2,0.6)-->jo jitna km hoga us pr true aaega\n",
    "    '''\n",
    "    for encodeFace, faceLoc in zip(encodeCurrFrame, faceCurrFrame):         \n",
    "        matches = face_recognition.compare_faces(encodeListKnown, encodeFace)\n",
    "        if len(matches) > 0:\n",
    "            print(\"matches\", matches[0])  # Print the first match\n",
    "        else:\n",
    "            print(\"No matches found\")\n",
    "            \n",
    "        faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)\n",
    "        if len(faceDis) > 0:\n",
    "            print(\"face distance\", faceDis[0])  # Print the first face distance\n",
    "        else:\n",
    "            print(\"No face distances found\")\n",
    "\n",
    "        matchIndex=np.argmin(faceDis)       # decimals ki jagha integers se frequesncy pta chlegi\n",
    "       # print(\"match Index:\",minIndex)\n",
    "    \n",
    "        if(matches[matchIndex]):\n",
    "            print(\"known face detected\")\n",
    "            '''\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    cv2.imshow(\"webcam\", img)\n",
    "    cv2.imshow(\"face identification\", imgBackground)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa691e35-b130-4c36-b4b2-dda3ba980444",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after 'for' statement on line 60 (423284191.py, line 61)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 61\u001b[1;36m\u001b[0m\n\u001b[1;33m    matches = face_recognition.compare_faces(encodeListKnown, encodeFace)  # Compare with all known face encodings\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block after 'for' statement on line 60\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(1)  # Change camera index as needed\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Couldn't open camera\")\n",
    "    exit()\n",
    "\n",
    "cap.set(3, 640)  # Width=640\n",
    "cap.set(4, 480)  # Height=480\n",
    "\n",
    "imgBackground = cv2.imread(r'C:\\Users\\user\\Desktop\\face recogination real db\\resources\\background.png')\n",
    "\n",
    "if imgBackground is None:\n",
    "    print(\"Couldn't load background image\")\n",
    "    exit()\n",
    "\n",
    "folderModePath = r\"C:\\Users\\user\\Desktop\\face recogination real db\\resources\\modes\"\n",
    "ModePath = os.listdir(folderModePath)\n",
    "imgModeList = []\n",
    "\n",
    "for path in ModePath:\n",
    "    img_mode = cv2.imread(os.path.join(folderModePath, path))\n",
    "    if img_mode is None:\n",
    "        print(f\"Couldn't load image: {path}\")\n",
    "        continue\n",
    "    img_mode_resized = cv2.resize(img_mode, (640, 480))\n",
    "    imgModeList.append(img_mode_resized)\n",
    "\n",
    "print(\"Number of loaded images:\", len(imgModeList))\n",
    "\n",
    "print(\"loading the encoding file\")\n",
    "file = open('encodeFile.p', 'rb')\n",
    "encodeListKnownWithIds = pickle.load(file)  # load hui file now extract\n",
    "file.close()\n",
    "encodeListKnown, studentId = encodeListKnownWithIds  # encode generator m dono ko ikthe kiya tha yha alg  for extraction\n",
    "print(\"the encoding file loaded\")\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        print(\"Couldn't read frame\")\n",
    "        break\n",
    "\n",
    "    imgSmall = cv2.resize(img, (0, 0), None, 0.25, 0.25)  # (0,0)-->pixel size 0.25,0.25 --> koi b size ho img ka use 1/4th krdo\n",
    "    imgSmall = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faceCurrFrame = face_recognition.face_locations(imgSmall)\n",
    "    encodeCurrFrame = face_recognition.face_encodings(imgSmall, faceCurrFrame)  # sare faces ki encoding ni krni curently available face ki krni h -->given faceCurrFrame\n",
    "    imgBackground[9:9 + 480, 3:3 + 640] = img\n",
    "\n",
    "    # Resize the image from imgModeList[1] to match the dimensions\n",
    "    img_mode_resized = cv2.resize(imgModeList[1], (414, 633))\n",
    "    imgBackground[44:44 + 633, 808:808 + 414] = img_mode_resized\n",
    "\n",
    "    for encodeFace, faceLoc in zip(encodeCurrFrame, faceCurrFrame):\n",
    "    matches = face_recognition.compare_faces(encodeListKnown, encodeFace)  # Compare with all known face encodings\n",
    "    faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)\n",
    "\n",
    "    for match, distance in zip(matches, faceDis):\n",
    "        print(\"Match:\", match)\n",
    "        print(\"Distance:\", distance)\n",
    "\n",
    "    # Get the index of the closest match\n",
    "    matchIndex = np.argmin(faceDis)\n",
    "\n",
    "    if matches[matchIndex]:\n",
    "        print(\"Known face detected\")\n",
    "\n",
    "    '''\n",
    "    for encodeFace, faceLoc in zip(encodeCurrFrame, faceCurrFrame):\n",
    "        matches = face_recognition.compare_faces(encodeListKnown, encodeFace)  # current face-->encodeFace mathces m compared face store hoga from list of known faces\n",
    "        faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)\n",
    "\n",
    "        print(\"matches\", matches)\n",
    "        print(\"face distance\", faceDis)\n",
    "\n",
    "        matchIndex = np.argmin(faceDis)  # decimals ki jagha integers se frequesncy pta chlegi\n",
    "    \n",
    "        if matches[matchIndex]:\n",
    "            print(\"known face detected\",)\n",
    "    \n",
    "     # frequency of matching faces (0.8,0.2,0.6)-->jo jitna km hoga us pr true aaega\n",
    "    \n",
    "    for encodeFace, faceLoc in zip(encodeCurrFrame, faceCurrFrame):         \n",
    "        matches = face_recognition.compare_faces(encodeListKnown, encodeFace)\n",
    "        if len(matches) > 0:\n",
    "            print(\"matches\", matches[0])  # Print the first match\n",
    "        else:\n",
    "            print(\"No matches found\")\n",
    "            \n",
    "        faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)\n",
    "        if len(faceDis) > 0:\n",
    "            print(\"face distance\", faceDis[0])  # Print the first face distance\n",
    "        else:\n",
    "            print(\"No face distances found\")\n",
    "\n",
    "        matchIndex=np.argmin(faceDis)       # decimals ki jagha integers se frequesncy pta chlegi\n",
    "       # print(\"match Index:\",minIndex)\n",
    "    \n",
    "        if(matches[matchIndex]):\n",
    "            print(\"known face detected\")\n",
    "            \n",
    "    '''\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    cv2.imshow(\"webcam\", img)\n",
    "    cv2.imshow(\"face identification\", imgBackground)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a7adfe3-e769-4d74-a6ae-00f69a969932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded images: 4\n",
      "loading the encoding file\n",
      "the encoding file loaded\n",
      "Match: False\n",
      "Distance: 0.8027251804062894\n",
      "Match: False\n",
      "Distance: 0.7960724643576526\n",
      "Match: False\n",
      "Distance: 0.7916394727556473\n",
      "Match: False\n",
      "Distance: 0.779773192157419\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(1)  # Change camera index as needed\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Couldn't open camera\")\n",
    "    exit()\n",
    "\n",
    "cap.set(3, 640)  # Width=640\n",
    "cap.set(4, 480)  # Height=480\n",
    "\n",
    "imgBackground = cv2.imread(r'C:\\Users\\user\\Desktop\\face recogination real db\\resources\\background.png')\n",
    "\n",
    "if imgBackground is None:\n",
    "    print(\"Couldn't load background image\")\n",
    "    exit()\n",
    "\n",
    "folderModePath = r\"C:\\Users\\user\\Desktop\\face recogination real db\\resources\\modes\"\n",
    "ModePath = os.listdir(folderModePath)\n",
    "imgModeList = []\n",
    "\n",
    "for path in ModePath:\n",
    "    img_mode = cv2.imread(os.path.join(folderModePath, path))\n",
    "    if img_mode is None:\n",
    "        print(f\"Couldn't load image: {path}\")\n",
    "        continue\n",
    "    img_mode_resized = cv2.resize(img_mode, (640, 480))\n",
    "    imgModeList.append(img_mode_resized)\n",
    "'''\n",
    "for path in ModePath:\n",
    "    img_mode = cv2.imread(os.path.join(folderModePath, path))\n",
    "    if img_mode is None:\n",
    "        print(f\"Couldn't load image: {path}\")\n",
    "    continue\n",
    "    img_mode_resized = cv2.resize(img_mode, (640, 480))\n",
    "    imgModeList.append(img_mode_resized)\n",
    "\n",
    "    # Encode the face in the current image and append to encodeListKnown\n",
    "    encodeFace = face_recognition.face_encodings(img_mode_resized)[0]  # Assuming there's only one face per image\n",
    "    encodeListKnown.append(encodeFace)\n",
    "'''\n",
    "print(\"Number of loaded images:\", len(imgModeList))\n",
    "\n",
    "print(\"loading the encoding file\")\n",
    "file = open('encodeFile.p', 'rb')\n",
    "encodeListKnownWithIds = pickle.load(file)  # load hui file now extract\n",
    "file.close()\n",
    "encodeListKnown, studentId = encodeListKnownWithIds  # encode generator m dono ko ikthe kiya tha yha alg  for extraction\n",
    "print(\"the encoding file loaded\")\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        print(\"Couldn't read frame\")\n",
    "        break\n",
    "\n",
    "    imgSmall = cv2.resize(img, (0, 0), None, 0.25, 0.25)  # (0,0)-->pixel size 0.25,0.25 --> koi b size ho img ka use 1/4th krdo\n",
    "    imgSmall = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faceCurrFrame = face_recognition.face_locations(imgSmall)\n",
    "    encodeCurrFrame = face_recognition.face_encodings(imgSmall, faceCurrFrame)  # sare faces ki encoding ni krni curently available face ki krni h -->given faceCurrFrame\n",
    "    imgBackground[162:162 + 480, 55:55 + 640] = img\n",
    "   \n",
    "    # Resize the image from imgModeList[1] to match the dimensions\n",
    "    img_mode_resized = cv2.resize(imgModeList[1], (414, 633))\n",
    "    imgBackground[44:44 + 633, 808:808 + 414] = img_mode_resized\n",
    "\n",
    "    for encodeFace, faceLoc in zip(encodeCurrFrame, faceCurrFrame):\n",
    "        matches = face_recognition.compare_faces(encodeListKnown, encodeFace)  # current face-->encodeFace mathces m compared face store hoga from list of known faces\n",
    "        faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)\n",
    "\n",
    "        for match, distance in zip(matches, faceDis):\n",
    "            print(\"Match:\", match)\n",
    "            print(\"Distance:\", distance)\n",
    "\n",
    "        # Get the index of the closest match\n",
    "        matchIndex = np.argmin(faceDis)\n",
    "\n",
    "        if matches[matchIndex]:\n",
    "            print(\"Known face detected\")\n",
    "            print(studentId[matchIndex])\n",
    "\n",
    "    # frequency of matching faces (0.8,0.2,0.6)-->jo jitna km hoga us pr true aaega\n",
    "    '''\n",
    "    for encodeFace, faceLoc in zip(encodeCurrFrame, faceCurrFrame):         \n",
    "        matches = face_recognition.compare_faces(encodeListKnown, encodeFace)\n",
    "        if len(matches) > 0:\n",
    "            print(\"matches\", matches[0])  # Print the first match\n",
    "        else:\n",
    "            print(\"No matches found\")\n",
    "            \n",
    "        faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)if len(faceDis) > 0:\n",
    "            print(\"face distance\", faceDis[0])  # Print the first face distance\n",
    "        else:\n",
    "            print(\"No face distances found\")\n",
    "\n",
    "        matchIndex=np.argmin(faceDis)       # decimals ki jagha integers se frequesncy pta chlegi\n",
    "       # print(\"match Index:\",minIndex)\n",
    "    \n",
    "        if(matches[matchIndex]):\n",
    "            print(\"known face detected\")\n",
    "            \n",
    "    '''\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    cv2.imshow(\"webcam\", img)\n",
    "    cv2.imshow(\"face identification\", imgBackground)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0799c4d1-eb68-4f0f-9cea-9f764281dfa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m     imgModeList\u001b[38;5;241m.\u001b[39mappend(img_mode_resized)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Encode the face in the current image and append to encodeListKnown\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m     encodeFace \u001b[38;5;241m=\u001b[39m \u001b[43mface_recognition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mface_encodings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_mode_resized\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Assuming there's only one face per image\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     encodeListKnown\u001b[38;5;241m.\u001b[39mappend(encodeFace)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of loaded images:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(imgModeList))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(1)  # Change camera index as needed\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Couldn't open camera\")\n",
    "    exit()\n",
    "\n",
    "cap.set(3, 640)  # Width=640\n",
    "cap.set(4, 480)  # Height=480\n",
    "\n",
    "imgBackground = cv2.imread(r'C:\\Users\\user\\Desktop\\face recogination real db\\resources\\background.png')\n",
    "\n",
    "if imgBackground is None:\n",
    "    print(\"Couldn't load background image\")\n",
    "    exit()\n",
    "\n",
    "folderModePath = r\"C:\\Users\\user\\Desktop\\face recogination real db\\resources\\modes\"\n",
    "ModePath = os.listdir(folderModePath)\n",
    "imgModeList = []\n",
    "\n",
    "for path in ModePath:\n",
    "    img_mode = cv2.imread(os.path.join(folderModePath, path))\n",
    "    if img_mode is None:\n",
    "        print(f\"Couldn't load image: {path}\")\n",
    "        continue\n",
    "    img_mode_resized = cv2.resize(img_mode, (640, 480))\n",
    "    imgModeList.append(img_mode_resized)\n",
    "\n",
    "    # Encode the face in the current image and append to encodeListKnown\n",
    "    encodeFace = face_recognition.face_encodings(img_mode_resized)[0]  # Assuming there's only one face per image\n",
    "    encodeListKnown.append(encodeFace)\n",
    "\n",
    "print(\"Number of loaded images:\", len(imgModeList))\n",
    "\n",
    "print(\"loading the encoding file\")\n",
    "file = open('encodeFile.p', 'rb')\n",
    "encodeListKnownWithIds = pickle.load(file)  # load hui file now extract\n",
    "file.close()\n",
    "encodeListKnown, studentId = encodeListKnownWithIds  # encode generator m dono ko ikthe kiya tha yha alg  for extraction\n",
    "print(\"the encoding file loaded\")\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        print(\"Couldn't read frame\")\n",
    "        break\n",
    "\n",
    "    imgSmall = cv2.resize(img, (0, 0), None, 0.25, 0.25)  # (0,0)-->pixel size 0.25,0.25 --> koi b size ho img ka use 1/4th krdo\n",
    "    imgSmall = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faceCurrFrame = face_recognition.face_locations(imgSmall)\n",
    "    encodeCurrFrame = face_recognition.face_encodings(imgSmall, faceCurrFrame)  # sare faces ki encoding ni krni curently available face ki krni h -->given faceCurrFrame\n",
    "    imgBackground[162:162 + 480, 55:55 + 640] = img\n",
    "   \n",
    "    # Resize the image from imgModeList[1] to match the dimensions\n",
    "    img_mode_resized = cv2.resize(imgModeList[1], (414, 633))\n",
    "    imgBackground[44:44 + 633, 808:808 + 414] = img_mode_resized\n",
    "\n",
    "    for encodeFace, faceLoc in zip(encodeCurrFrame, faceCurrFrame):\n",
    "        matches = face_recognition.compare_faces(encodeListKnown, encodeFace)  # current face-->encodeFace mathces m compared face store hoga from list of known faces\n",
    "        faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)\n",
    "\n",
    "        for match, distance in zip(matches, faceDis):\n",
    "            print(\"Match:\", match)\n",
    "            print(\"Distance:\", distance)\n",
    "\n",
    "        # Get the index of the closest match\n",
    "        matchIndex = np.argmin(faceDis)\n",
    "\n",
    "        if matches[matchIndex]:\n",
    "            print(\"Known face detected\")\n",
    "            print(studentId[matchIndex])\n",
    "\n",
    "    # frequency of matching faces (0.8,0.2,0.6)-->jo jitna km hoga us pr true aaega\n",
    "    '''\n",
    "    for encodeFace, faceLoc in zip(encodeCurrFrame, faceCurrFrame):         \n",
    "        matches = face_recognition.compare_faces(encodeListKnown, encodeFace)\n",
    "        if len(matches) > 0:\n",
    "            print(\"matches\", matches[0])  # Print the first match\n",
    "        else:\n",
    "            print(\"No matches found\")\n",
    "            \n",
    "        faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)if len(faceDis) > 0:\n",
    "            print(\"face distance\", faceDis[0])  # Print the first face distance\n",
    "        else:\n",
    "            print(\"No face distances found\")\n",
    "\n",
    "        matchIndex=np.argmin(faceDis)       # decimals ki jagha integers se frequesncy pta chlegi\n",
    "       # print(\"match Index:\",minIndex)\n",
    "    \n",
    "        if(matches[matchIndex]):\n",
    "            print(\"known face detected\")\n",
    "            \n",
    "    '''\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    cv2.imshow(\"webcam\", img)\n",
    "    cv2.imshow(\"face identification\", imgBackground)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf65ea58-b86b-48ff-ba5c-6d8f3f42283e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded images: 4\n",
      "loading the encoding file\n",
      "the encoding file loaded\n",
      "Match: False\n",
      "Distance: 0.746034187795091\n",
      "Match: False\n",
      "Distance: 0.7400989004864286\n",
      "Match: False\n",
      "Distance: 0.7479934073180732\n",
      "Match: False\n",
      "Distance: 0.7201138329803818\n",
      "Match: False\n",
      "Distance: 0.7124155195638269\n",
      "Match: False\n",
      "Distance: 0.7501285078378205\n",
      "Match: False\n",
      "Distance: 0.7410168975188736\n",
      "Match: False\n",
      "Distance: 0.7521706153043195\n",
      "Match: False\n",
      "Distance: 0.721030920877234\n",
      "Match: False\n",
      "Distance: 0.7135802277231993\n",
      "Match: False\n",
      "Distance: 0.7213027269101553\n",
      "Match: False\n",
      "Distance: 0.7333460289216902\n",
      "Match: False\n",
      "Distance: 0.7960155898928315\n",
      "Match: False\n",
      "Distance: 0.7235598469792486\n",
      "Match: False\n",
      "Distance: 0.7148603890889629\n",
      "Match: False\n",
      "Distance: 0.7433217506079911\n",
      "Match: False\n",
      "Distance: 0.7203194536792055\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import cvzone\n",
    "import pickle\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "\n",
    "cap = cv2.VideoCapture(1)  # Change camera index as needed\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Couldn't open camera\")\n",
    "    exit()\n",
    "\n",
    "cap.set(3, 640)  # Width=640\n",
    "cap.set(4, 480)  # Height=480\n",
    "\n",
    "imgBackground = cv2.imread(r'C:\\Users\\user\\Desktop\\face recogination real db\\resources\\background.png')\n",
    "\n",
    "if imgBackground is None:\n",
    "    print(\"Couldn't load background image\")\n",
    "    exit()\n",
    "\n",
    "folderModePath = r\"C:\\Users\\user\\Desktop\\face recogination real db\\resources\\modes\"\n",
    "ModePath = os.listdir(folderModePath)\n",
    "imgModeList = []\n",
    "encodeListKnown = []\n",
    "studentId = []\n",
    "\n",
    "for path in ModePath:\n",
    "    img_mode = cv2.imread(os.path.join(folderModePath, path))\n",
    "    if img_mode is None:\n",
    "        print(f\"Couldn't load image: {path}\")\n",
    "        continue\n",
    "    img_mode_resized = cv2.resize(img_mode, (640, 480))\n",
    "    imgModeList.append(img_mode_resized)\n",
    "\n",
    "    # Encode the face in the current image and append to encodeListKnown\n",
    "    face_encodings = face_recognition.face_encodings(img_mode_resized)\n",
    "    if len(face_encodings) > 0:\n",
    "        encodeListKnown.append(face_encodings[0])  # Assuming there's only one face per image\n",
    "        studentId.append(path.split('.')[0])  # Extract student ID from the filename\n",
    "\n",
    "print(\"Number of loaded images:\", len(imgModeList))\n",
    "\n",
    "print(\"loading the encoding file\")\n",
    "file = open('encodeFile.p', 'rb')\n",
    "encodeListKnownWithIds = pickle.load(file)  # load hui file now extract\n",
    "file.close()\n",
    "encodeListKnown.extend(encodeListKnownWithIds[0])  # Append previously loaded encodings\n",
    "studentId.extend(encodeListKnownWithIds[1])  # Append previously loaded student IDs\n",
    "print(\"the encoding file loaded\")\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        print(\"Couldn't read frame\")\n",
    "        break\n",
    "\n",
    "    imgSmall = cv2.resize(img, (0, 0), None, 0.25, 0.25)  # (0,0)-->pixel size 0.25,0.25 --> koi b size ho img ka use 1/4th krdo\n",
    "    imgSmall = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faceCurrFrame = face_recognition.face_locations(imgSmall)\n",
    "    encodeCurrFrame = face_recognition.face_encodings(imgSmall, faceCurrFrame)  # sare faces ki encoding ni krni curently available face ki krni h -->given faceCurrFrame\n",
    "    imgBackground[162:162 + 480, 55:55 + 640] = img\n",
    "   \n",
    "    # Resize the image from imgModeList[1] to match the dimensions\n",
    "    img_mode_resized = cv2.resize(imgModeList[1], (414, 633))\n",
    "    imgBackground[44:44 + 633, 808:808 + 414] = img_mode_resized\n",
    "\n",
    "    for encodeFace, faceLoc in zip(encodeCurrFrame, faceCurrFrame):\n",
    "        matches = face_recognition.compare_faces(encodeListKnown, encodeFace)  # current face-->encodeFace mathces m compared face store hoga from list of known faces\n",
    "        faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)\n",
    "\n",
    "        for match, distance in zip(matches, faceDis):\n",
    "            print(\"Match:\", match)\n",
    "            print(\"Distance:\", distance)\n",
    "\n",
    "        # Get the index of the closest match\n",
    "        matchIndex = np.argmin(faceDis)\n",
    "\n",
    "        if matches[matchIndex]:\n",
    "            #print(\"Known face detected\")\n",
    "            #print(studentId[matchIndex])\n",
    "\n",
    "            \n",
    "            # provinding bounding box information from facelocation --->y1, x2, y2, x1 \n",
    "            y1, x2, y2, x1 = faceLoc     \n",
    "            y1, x2, y2, x1 =y1*4, x2*4, y2*4, x1*4       # *4 coz we reduced size from 1/4th \n",
    "            bbox = 55+x1, 162+y1, x2-x1, y2-y1    # bounding box -->image set hojaegi \n",
    "            imgBackground = cvzone.cornerRect(imgBackground,bbox,rt=0)  # detected image pr box aage--->boundarybox-->rt rectangleWidth\n",
    "    # frequency of matching faces (0.8,0.2,0.6)-->jo jitna km hoga us pr true aaega\n",
    "    '''\n",
    "    for encodeFace, faceLoc in zip(encodeCurrFrame, faceCurrFrame):         \n",
    "        matches = face_recognition.compare_faces(encodeListKnown, encodeFace)\n",
    "        if len(matches) > 0:\n",
    "            print(\"matches\", matches[0])  # Print the first match\n",
    "        else:\n",
    "            print(\"No matches found\")\n",
    "            \n",
    "        faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)if len(faceDis) > 0:\n",
    "            print(\"face distance\", faceDis[0])  # Print the first face distance\n",
    "        else:\n",
    "            print(\"No face distances found\")\n",
    "\n",
    "        matchIndex=np.argmin(faceDis)       # decimals ki jagha integers se frequesncy pta chlegi\n",
    "       # print(\"match Index:\",minIndex)\n",
    "    \n",
    "        if(matches[matchIndex]):\n",
    "            print(\"known face detected\")\n",
    "            \n",
    "    '''\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    #cv2.imshow(\"webcam\", img)\n",
    "    cv2.imshow(\"face identification\", imgBackground)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba5f127-fc49-461b-a96d-a72c6ae17693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded images: 4\n",
      "loading the encoding file\n",
      "the encoding file loaded\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (480,640,3) into shape (633,414,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 95\u001b[0m\n\u001b[0;32m     87\u001b[0m imgBackground[\u001b[38;5;241m162\u001b[39m:\u001b[38;5;241m162\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m480\u001b[39m, \u001b[38;5;241m55\u001b[39m:\u001b[38;5;241m55\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m640\u001b[39m] \u001b[38;5;241m=\u001b[39m img\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# Resize the image from imgModeList[1] to match the dimensions\u001b[39;00m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m#img_mode_resized = cv2.resize(imgModeList[1], (414, 633))\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m#imgBackground[44:44 + 633, 808:808 + 414] =img_mode_resized \u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m#imgModeList[0]= img_mode_resized \u001b[39;00m\n\u001b[1;32m---> 95\u001b[0m \u001b[43mimgBackground\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m44\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m44\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m633\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m808\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m808\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m414\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m imgModeList[modeType]  \u001b[38;5;66;03m# Assign the selected image to the background\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m encodeFace, faceLoc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(encodeCurrFrame, faceCurrFrame):\n\u001b[0;32m     99\u001b[0m     matches \u001b[38;5;241m=\u001b[39m face_recognition\u001b[38;5;241m.\u001b[39mcompare_faces(encodeListKnown, encodeFace)  \u001b[38;5;66;03m# current face-->encodeFace mathces m compared face store hoga from list of known faces\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (480,640,3) into shape (633,414,3)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import cvzone\n",
    "import pickle\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, initialize_app, storage\n",
    "import pyrebase\n",
    "import pickle\n",
    "\n",
    "# Check if Firebase app is already initialized\n",
    "try:\n",
    "    firebase_admin.get_app(name='my_app')\n",
    "except ValueError as e:\n",
    "    # If Firebase app is not initialized, initialize it with a unique name\n",
    "    cred = credentials.Certificate(\"C:\\\\Users\\\\user\\\\Downloads\\\\votingService key.json\")\n",
    "    initialize_app(cred, {\n",
    "        \"databaseURL\": \"https://votingfacerecognition-default-rtdb.firebaseio.com/\",\n",
    "        \"storageBucket\": 'votingfacerecognition.appspot.com'\n",
    "    }, name='my_app')\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(1)  # Change camera index as needed\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Couldn't open camera\")\n",
    "    exit()\n",
    "\n",
    "cap.set(3, 640)  # Width=640\n",
    "cap.set(4, 480)  # Height=480\n",
    "\n",
    "imgBackground = cv2.imread(r'C:\\Users\\user\\Desktop\\face recogination real db\\resources\\background.png')\n",
    "\n",
    "if imgBackground is None:\n",
    "    print(\"Couldn't load background image\")\n",
    "    exit()\n",
    "\n",
    "folderModePath = r\"C:\\Users\\user\\Desktop\\face recogination real db\\resources\\modes\"\n",
    "ModePath = os.listdir(folderModePath)\n",
    "imgModeList = []\n",
    "encodeListKnown = []\n",
    "studentId = []\n",
    "\n",
    "for path in ModePath:\n",
    "    img_mode = cv2.imread(os.path.join(folderModePath, path))\n",
    "    if img_mode is None:\n",
    "        print(f\"Couldn't load image: {path}\")\n",
    "        continue\n",
    "    img_mode_resized = cv2.resize(img_mode, (640, 480))\n",
    "    imgModeList.append(img_mode_resized)\n",
    "\n",
    "    # Encode the face in the current image and append to encodeListKnown\n",
    "    face_encodings = face_recognition.face_encodings(img_mode_resized)\n",
    "    if len(face_encodings) > 0:\n",
    "        encodeListKnown.append(face_encodings[0])  # Assuming there's only one face per image\n",
    "        studentId.append(path.split('.')[0])  # Extract student ID from the filename\n",
    "\n",
    "print(\"Number of loaded images:\", len(imgModeList))\n",
    "\n",
    "print(\"loading the encoding file\")\n",
    "file = open('encodeFile.p', 'rb')\n",
    "encodeListKnownWithIds = pickle.load(file)  # load hui file now extract\n",
    "file.close()\n",
    "encodeListKnown.extend(encodeListKnownWithIds[0])  # Append previously loaded encodings\n",
    "studentId.extend(encodeListKnownWithIds[1])  # Append previously loaded student IDs\n",
    "print(\"the encoding file loaded\")\n",
    "\n",
    "\n",
    "modeType = 0\n",
    "\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        print(\"Couldn't read frame\")\n",
    "        break\n",
    "\n",
    "    imgSmall = cv2.resize(img, (0, 0), None, 0.25, 0.25)  # (0,0)-->pixel size 0.25,0.25 --> koi b size ho img ka use 1/4th krdo\n",
    "    imgSmall = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faceCurrFrame = face_recognition.face_locations(imgSmall)\n",
    "    encodeCurrFrame = face_recognition.face_encodings(imgSmall, faceCurrFrame)  # sare faces ki encoding ni krni curently available face ki krni h -->given faceCurrFrame\n",
    "   \n",
    "    imgBackground[162:162 + 480, 55:55 + 640] = img\n",
    "   \n",
    "    # Resize the image from imgModeList[1] to match the dimensions\n",
    "    #img_mode_resized = cv2.resize(imgModeList[1], (414, 633))\n",
    "    \n",
    "    #imgBackground[44:44 + 633, 808:808 + 414] =img_mode_resized \n",
    "    #imgModeList[0]= img_mode_resized \n",
    "\n",
    "    imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]  # Assign the selected image to the background\n",
    "\n",
    "    \n",
    "    for encodeFace, faceLoc in zip(encodeCurrFrame, faceCurrFrame):\n",
    "        matches = face_recognition.compare_faces(encodeListKnown, encodeFace)  # current face-->encodeFace mathces m compared face store hoga from list of known faces\n",
    "        faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)\n",
    "\n",
    "        for match, distance in zip(matches, faceDis):\n",
    "            print(\"Match:\", match)\n",
    "            print(\"Distance:\", distance)\n",
    "\n",
    "        # Get the index of the closest match\n",
    "        matchIndex = np.argmin(faceDis)\n",
    "\n",
    "        if matches[matchIndex]:\n",
    "            #print(\"Known face detected\")\n",
    "            #print(studentId[matchIndex])\n",
    "\n",
    "            \n",
    "            # provinding bounding box information from facelocation --->y1, x2, y2, x1 \n",
    "            y1, x2, y2, x1 = faceLoc     \n",
    "            y1, x2, y2, x1 =y1*4, x2*4, y2*4, x1*4       # *4 coz we reduced size from 1/4th \n",
    "            bbox = 55+x1, 162+y1, x2-x1, y2-y1    # bounding box -->image set hojaegi \n",
    "            imgBackground = cvzone.cornerRect(imgBackground,bbox,rt=0)  # detected image pr box aage--->boundarybox-->rt rectangleWidth\n",
    "\n",
    "\n",
    "            id= studentId[matchIndex]\n",
    "            if counter == 0:\n",
    "                counter = 1\n",
    "                modeType = 1\n",
    "    if counter !=0:\n",
    "\n",
    "        if counter==1:\n",
    "            #getting the data\n",
    "\n",
    "            studentInfo = db.reference(f'students/{id}').get()\n",
    "            print(studentInfo)\n",
    "            # get image from the storage\n",
    "            blob=bucket.get_blob(f'images/{id}.png')\n",
    "            array= np.frombuffer(blob.download_as_string(), np.vint8)\n",
    "            imgStudent = cv2.indercode(array,cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "        cv2.putText(imgBackground, str(studentInfo['total_attendance']), (861,125),\n",
    "                    cv2.FONT_HERSHEY_COMPLEX, 1, (255,255,255),1)\n",
    "#while(\n",
    "    # frequency of matching faces (0.8,0.2,0.6)-->jo jitna km hoga us pr true aaega\n",
    "    '''\n",
    "    for encodeFace, faceLoc in zip(encodeCurrFrame, faceCurrFrame):         \n",
    "        matches = face_recognition.compare_faces(encodeListKnown, encodeFace)\n",
    "        if len(matches) > 0:\n",
    "            print(\"matches\", matches[0])  # Print the first match\n",
    "        else:\n",
    "            print(\"No matches found\")\n",
    "            \n",
    "        faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)if len(faceDis) > 0:\n",
    "            print(\"face distance\", faceDis[0])  # Print the first face distance\n",
    "        else:\n",
    "            print(\"No face distances found\")\n",
    "\n",
    "        matchIndex=np.argmin(faceDis)       # decimals ki jagha integers se frequesncy pta chlegi\n",
    "       # print(\"match Index:\",minIndex)\n",
    "    \n",
    "        if(matches[matchIndex]):\n",
    "            print(\"known face detected\")\n",
    "            \n",
    "    '''\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    #cv2.imshow(\"webcam\", img)\n",
    "    cv2.imshow(\"face identification\", imgBackground)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d6b93e64-54ca-466c-8074-168501294687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded images: 4\n",
      "loading the encoding file\n",
      "the encoding file loaded\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (480,640,3) into shape (633,414,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 90\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m#imgBackground[162:162 + 480, 55:55 + 640] = img\u001b[39;00m\n\u001b[0;32m     89\u001b[0m imgBackground[\u001b[38;5;241m162\u001b[39m:\u001b[38;5;241m162\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m480\u001b[39m, \u001b[38;5;241m55\u001b[39m:\u001b[38;5;241m55\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m640\u001b[39m] \u001b[38;5;241m=\u001b[39m img\n\u001b[1;32m---> 90\u001b[0m \u001b[43mimgBackground\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m44\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m44\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m633\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m808\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m808\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m414\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m imgModeList[modeType]\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Resize the image from imgModeList[1] to match the dimensions\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m#img_mode_resized = cv2.resize(imgModeList[1], (414, 633))\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    100\u001b[0m \n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m#imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]  # Assign the selected image to the background\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m FaceCurrFrame:    \n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (480,640,3) into shape (633,414,3)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import cvzone\n",
    "import pickle\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, initialize_app, storage\n",
    "import pyrebase\n",
    "import pickle\n",
    "\n",
    "# Check if Firebase app is already initialized\n",
    "try:\n",
    "    firebase_admin.get_app(name='my_app')\n",
    "except ValueError as e:\n",
    "    # If Firebase app is not initialized, initialize it with a unique name\n",
    "    cred = credentials.Certificate(\"C:\\\\Users\\\\user\\\\Downloads\\\\votingService key.json\")\n",
    "    initialize_app(cred, {\n",
    "        \"databaseURL\": \"https://votingfacerecognition-default-rtdb.firebaseio.com/\",\n",
    "        \"storageBucket\": 'votingfacerecognition.appspot.com'\n",
    "    }, name='my_app')\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(1)  # Change camera index as needed\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Couldn't open camera\")\n",
    "    exit()\n",
    "\n",
    "cap.set(3, 640)  # Width=640\n",
    "cap.set(4, 480)  # Height=480\n",
    "\n",
    "imgBackground = cv2.imread(r'C:\\Users\\user\\Desktop\\face recogination real db\\resources\\background.png')\n",
    "\n",
    "if imgBackground is None:\n",
    "    print(\"Couldn't load background image\")\n",
    "    exit()\n",
    "\n",
    "folderModePath = r\"C:\\Users\\user\\Desktop\\face recogination real db\\resources\\modes\"\n",
    "ModePath = os.listdir(folderModePath)\n",
    "imgModeList = []\n",
    "encodeListKnown = []\n",
    "studentId = []\n",
    "\n",
    "for path in ModePath:\n",
    "    img_mode = cv2.imread(os.path.join(folderModePath, path))\n",
    "    if img_mode is None:\n",
    "        print(f\"Couldn't load image: {path}\")\n",
    "        continue\n",
    "    img_mode_resized = cv2.resize(img_mode, (640, 480))\n",
    "    imgModeList.append(img_mode_resized)\n",
    "\n",
    "    # Encode the face in the current image and append to encodeListKnown\n",
    "    face_encodings = face_recognition.face_encodings(img_mode_resized)\n",
    "    if len(face_encodings) > 0:\n",
    "        encodeListKnown.append(face_encodings[0])  # Assuming there's only one face per image\n",
    "        studentId.append(path.split('.')[0])  # Extract student ID from the filename\n",
    "\n",
    "print(\"Number of loaded images:\", len(imgModeList))\n",
    "\n",
    "print(\"loading the encoding file\")\n",
    "file = open('encodeFile.p', 'rb')\n",
    "encodeListKnownWithIds = pickle.load(file)  # load hui file now extract\n",
    "file.close()\n",
    "encodeListKnown.extend(encodeListKnownWithIds[0])  # Append previously loaded encodings\n",
    "studentId.extend(encodeListKnownWithIds[1])  # Append previously loaded student IDs\n",
    "print(\"the encoding file loaded\")\n",
    "\n",
    "\n",
    "modeType = 0\n",
    "counter=0\n",
    "id = -1\n",
    "imgStudent = []\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        print(\"Couldn't read frame\")\n",
    "        break\n",
    "\n",
    "    imgSmall = cv2.resize(img, (0, 0), None, 0.25, 0.25)  # (0,0)-->pixel size 0.25,0.25 --> koi b size ho img ka use 1/4th krdo\n",
    "    imgSmall = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faceCurrFrame = face_recognition.face_locations(imgSmall)\n",
    "    encodeCurrFrame = face_recognition.face_encodings(imgSmall, faceCurrFrame)  # sare faces ki encoding ni krni curently available face ki krni h -->given faceCurrFrame\n",
    "   \n",
    "    #imgBackground[162:162 + 480, 55:55 + 640] = img\n",
    "    imgBackground[162:162 + 480, 55:55 + 640] = img\n",
    "    imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    "\n",
    "   \n",
    "    # Resize the image from imgModeList[1] to match the dimensions\n",
    "    #img_mode_resized = cv2.resize(imgModeList[1], (414, 633))\n",
    "    \n",
    "    #imgBackground[44:44 + 633, 808:808 + 414] =img_mode_resized \n",
    "    #imgModeList[0]= img_mode_resized \n",
    "   # imgBackground[44:44 + 633, 808:808 + 414] = cv2.resize(imgModeList[modeType], (414, 633))\n",
    "\n",
    "\n",
    "    #imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]  # Assign the selected image to the background\n",
    "\n",
    "    if FaceCurrFrame:    \n",
    "        for encodeFace, faceLoc in zip(encodeCurrFrame, faceCurrFrame):\n",
    "            matches = face_recognition.compare_faces(encodeListKnown, encodeFace)  # current face-->encodeFace mathces m compared face store hoga from list of known faces\n",
    "            faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)\n",
    "    \n",
    "            for match, distance in zip(matches, faceDis):\n",
    "                print(\"Match:\", match)\n",
    "                print(\"Distance:\", distance)\n",
    "    \n",
    "            # Get the index of the closest match\n",
    "            matchIndex = np.argmin(faceDis)\n",
    "    \n",
    "            if matches[matchIndex]:\n",
    "                #print(\"Known face detected\")\n",
    "                #print(studentId[matchIndex])\n",
    "    \n",
    "                \n",
    "                # provinding bounding box information from facelocation --->y1, x2, y2, x1 \n",
    "                y1, x2, y2, x1 = faceLoc     \n",
    "                y1, x2, y2, x1 =y1*4, x2*4, y2*4, x1*4       # *4 coz we reduced size from 1/4th \n",
    "                bbox = 55+x1, 162+y1, x2-x1, y2-y1    # bounding box -->image set hojaegi \n",
    "                imgBackground = cvzone.cornerRect(imgBackground,bbox,rt=0)  # detected image pr box aage--->boundarybox-->rt rectangleWidth\n",
    "    \n",
    "    \n",
    "                id= studentId[matchIndex]\n",
    "                if counter == 0:\n",
    "                    counter = 1\n",
    "                    modeType = 1\n",
    "        if counter !=0:\n",
    "    \n",
    "            if counter==1:\n",
    "                #getting the data\n",
    "    \n",
    "                studentInfo = db.reference(f'students/{id}').get()\n",
    "                print(studentInfo)\n",
    "                # get image from the storage\n",
    "                blob=bucket.get_blob(f'images/{id}.png')\n",
    "                array= np.frombuffer(blob.download_as_string(), np.vint8)\n",
    "                imgStudent = cv2.indercode(array,cv2.COLOR_BGRA2BGR)\n",
    "                \n",
    "                #update data of voting\n",
    "                datetimeObject = datetime.strptime(studentInfo['last_attendance_time'],\n",
    "                                                   \"%Y-%m-%d %H:%M:%S\")\n",
    "                secondsElapsed = (datetime.now() - datetimeObject).total_seconds()\n",
    "                print(secondsElapsed)\n",
    "                if secondsElapsed > 30:\n",
    "                    ref = db.reference(f'Students/{id}')\n",
    "                    studentInfo['total_attendance'] += 1\n",
    "                    ref.child('total_attendance').set(studentInfo['total_attendance'])\n",
    "                    ref.child('last_attendance_time').set(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                else:\n",
    "                    modeType = 3\n",
    "                    counter = 0\n",
    "                    imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    "\n",
    "    \n",
    "                # updating data into server\n",
    "                ref.child('total_attendance').set(studentInfo['total_attendance'])\n",
    "            \n",
    "            if modeType != 3:\n",
    "\n",
    "                if 10 < counter < 20:\n",
    "                    modeType = 2\n",
    "\n",
    "            imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    "\n",
    "            if counter <= 10:\n",
    "                cv2.putText(imgBackground, str(studentInfo['total attendance']), (861,125),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 1, (255,255,255),1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['major']), (1006,550),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.5, (255,255,255),1)\n",
    "                cv2.putText(imgBackground, str(id), (1006, 493),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.5, (255,255,255),1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['standing']), (910, 625),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.6, (100,100,100), 1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['year']), (1025,625),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.6, (100,100,100),1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['starting year']), (1125,625),\n",
    "                             cv2.FONT_HERSHEY_COMPLEX, 0.6, (100,100,100),1)\n",
    "                \n",
    "        \n",
    "        \n",
    "                (w,h), _=cv2.getTextSize(stundentInfo['name'],cv2.FONT_HERSHEY_COMPLEX,  1, 1)\n",
    "                offset=(414 -w)//2\n",
    "                cv2.putText(imgBAckground, str(studentInfo['name']),(808+offset, 445),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 1, (50,50,50),1)\n",
    "        \n",
    "                imgBackground[175:175+216,909:909+216]=imgStudent\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            if counter >= 20:\n",
    "                counter = 0\n",
    "                modeType = 0\n",
    "                studentInfo = []\n",
    "                imgStudent = []\n",
    "                imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    " \n",
    "    else:\n",
    "         modeType = 0\n",
    "         counter = 0\n",
    "        \n",
    "        \n",
    "    #cv2.imshow(\"webcam\", img)\n",
    "    cv2.imshow(\"face identification\", imgBackground)    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5251786-394f-42c2-9f91-a57729882cff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded images: 4\n",
      "loading the encoding file\n",
      "the encoding file loaded\n",
      "Match: False\n",
      "Distance: 0.8430824895690497\n",
      "Match: True\n",
      "Distance: 0.502119617732676\n",
      "Match: True\n",
      "Distance: 0.536535708042232\n",
      "Match: True\n",
      "Distance: 0.4419272913886295\n",
      "Match: False\n",
      "Distance: 0.8407518231030887\n",
      "Match: False\n",
      "Distance: 0.7933260651105482\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The default Firebase app does not exist. Make sure to initialize the SDK by calling initialize_app().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 130\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m counter \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m counter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;66;03m#getting the data\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m         studentInfo \u001b[38;5;241m=\u001b[39m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstudents/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget()\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;28mprint\u001b[39m(studentInfo)\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;66;03m# get image from the storage\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\firebase_admin\\db.py:68\u001b[0m, in \u001b[0;36mreference\u001b[1;34m(path, app, url)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreference\u001b[39m(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, app\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a database ``Reference`` representing the node at the specified path.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    If no path is specified, this function returns a ``Reference`` that represents the database\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m      ValueError: If the specified path or app is invalid.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     service \u001b[38;5;241m=\u001b[39m \u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_app_service\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_DB_ATTRIBUTE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_DatabaseService\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     client \u001b[38;5;241m=\u001b[39m service\u001b[38;5;241m.\u001b[39mget_client(url)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Reference(client\u001b[38;5;241m=\u001b[39mclient, path\u001b[38;5;241m=\u001b[39mpath)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\firebase_admin\\_utils.py:97\u001b[0m, in \u001b[0;36mget_app_service\u001b[1;34m(app, name, initializer)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_app_service\u001b[39m(app, name, initializer):\n\u001b[1;32m---> 97\u001b[0m     app \u001b[38;5;241m=\u001b[39m \u001b[43m_get_initialized_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m app\u001b[38;5;241m.\u001b[39m_get_service(name, initializer)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\firebase_admin\\_utils.py:82\u001b[0m, in \u001b[0;36m_get_initialized_app\u001b[1;34m(app)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a reference to an initialized App instance.\"\"\"\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m app \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfirebase_admin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(app, firebase_admin\u001b[38;5;241m.\u001b[39mApp):\n\u001b[0;32m     85\u001b[0m     initialized_app \u001b[38;5;241m=\u001b[39m firebase_admin\u001b[38;5;241m.\u001b[39mget_app(app\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\firebase_admin\\__init__.py:137\u001b[0m, in \u001b[0;36mget_app\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _apps[name]\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m _DEFAULT_APP_NAME:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe default Firebase app does not exist. Make sure to initialize \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe SDK by calling initialize_app().\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    142\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFirebase app named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m does not exist. Make sure to initialize \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    143\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe SDK by calling initialize_app() with your app name as the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    144\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecond argument.\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(name))\n",
      "\u001b[1;31mValueError\u001b[0m: The default Firebase app does not exist. Make sure to initialize the SDK by calling initialize_app()."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import cvzone\n",
    "import pickle\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, initialize_app, storage,db\n",
    "\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials\n",
    "from firebase_admin import db\n",
    "\n",
    "\n",
    "import pyrebase\n",
    "import pickle\n",
    "\n",
    "# Check if Firebase app is already initialized\n",
    "try:\n",
    "    firebase_admin.get_app(name='my_app')\n",
    "except ValueError as e:\n",
    "    # If Firebase app is not initialized, initialize it with a unique name\n",
    "    cred = credentials.Certificate(\"C:\\\\Users\\\\user\\\\Downloads\\\\votingService key.json\")\n",
    "    initialize_app(cred, {\n",
    "        \"databaseURL\": \"https://votingfacerecognition-default-rtdb.firebaseio.com/\",\n",
    "        \"storageBucket\": 'votingfacerecognition.appspot.com'\n",
    "    }, name='my_app')\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(1)  # Change camera index as needed\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Couldn't open camera\")\n",
    "    exit()\n",
    "\n",
    "cap.set(3, 640)  # Width=640\n",
    "cap.set(4, 480)  # Height=480\n",
    "\n",
    "imgBackground = cv2.imread(r'C:\\Users\\user\\Desktop\\face recogination real db\\resources\\background.png')\n",
    "\n",
    "if imgBackground is None:\n",
    "    print(\"Couldn't load background image\")\n",
    "    exit()\n",
    "\n",
    "folderModePath = r\"C:\\Users\\user\\Desktop\\face recogination real db\\resources\\modes\"\n",
    "ModePath = os.listdir(folderModePath)\n",
    "imgModeList = []\n",
    "encodeListKnown = []\n",
    "studentId = []\n",
    "\n",
    "for path in ModePath:\n",
    "    img_mode = cv2.imread(os.path.join(folderModePath, path))\n",
    "    if img_mode is None:\n",
    "        print(f\"Couldn't load image: {path}\")\n",
    "        continue\n",
    "    imgModeList.append(img_mode)\n",
    "\n",
    "    # Encode the face in the current image and append to encodeListKnown\n",
    "    face_encodings = face_recognition.face_encodings(img_mode)\n",
    "    if len(face_encodings) > 0:\n",
    "        encodeListKnown.append(face_encodings[0])  # Assuming there's only one face per image\n",
    "        studentId.append(path.split('.')[0])  # Extract student ID from the filename\n",
    "\n",
    "print(\"Number of loaded images:\", len(imgModeList))\n",
    "\n",
    "print(\"loading the encoding file\")\n",
    "file = open('encodeFile.p', 'rb')\n",
    "encodeListKnownWithIds = pickle.load(file)  # load hui file now extract\n",
    "file.close()\n",
    "encodeListKnown.extend(encodeListKnownWithIds[0])  # Append previously loaded encodings\n",
    "studentId.extend(encodeListKnownWithIds[1])  # Append previously loaded student IDs\n",
    "print(\"the encoding file loaded\")\n",
    "\n",
    "\n",
    "modeType = 0\n",
    "counter=0\n",
    "id = -1\n",
    "imgStudent = []\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        print(\"Couldn't read frame\")\n",
    "        break\n",
    "\n",
    "    imgSmall = cv2.resize(img, (0, 0), None, 0.25, 0.25)  # (0,0)-->pixel size 0.25,0.25 --> koi b size ho img ka use 1/4th krdo\n",
    "    imgSmall = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faceCurrFrame = face_recognition.face_locations(imgSmall)\n",
    "    encodeCurrFrame = face_recognition.face_encodings(imgSmall, faceCurrFrame)  # sare faces ki encoding ni krni curently available face ki krni h -->given faceCurrFrame\n",
    "   \n",
    "    #imgBackground[162:162 + 480, 55:55 + 640] = img\n",
    "    imgBackground[162:162 + 480, 55:55 + 640] = img\n",
    "    imgBackground[44:44 + imgModeList[modeType].shape[0], 808:808 + imgModeList[modeType].shape[1]] = imgModeList[modeType]\n",
    "\n",
    "    if faceCurrFrame:    \n",
    "        for encodeFace, faceLoc in zip(encodeCurrFrame, faceCurrFrame):\n",
    "            matches = face_recognition.compare_faces(encodeListKnown, encodeFace)  # current face-->encodeFace mathces m compared face store hoga from list of known faces\n",
    "            faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)\n",
    "    \n",
    "            for match, distance in zip(matches, faceDis):\n",
    "                print(\"Match:\", match)\n",
    "                print(\"Distance:\", distance)\n",
    "    \n",
    "            # Get the index of the closest match\n",
    "            matchIndex = np.argmin(faceDis)\n",
    "    \n",
    "            if matches[matchIndex]:\n",
    "                #print(\"Known face detected\")\n",
    "                #print(studentId[matchIndex])\n",
    "    \n",
    "                \n",
    "                # provinding bounding box information from facelocation --->y1, x2, y2, x1 \n",
    "                y1, x2, y2, x1 = faceLoc     \n",
    "                y1, x2, y2, x1 = y1*4, x2*4, y2*4, x1*4       # *4 coz we reduced size from 1/4th \n",
    "                bbox = 55+x1, 162+y1, x2-x1, y2-y1    # bounding box -->image set hojaegi \n",
    "                imgBackground = cvzone.cornerRect(imgBackground,bbox,rt=0)  # detected image pr box aage--->boundarybox-->rt rectangleWidth\n",
    "    \n",
    "    \n",
    "                id= studentId[matchIndex]\n",
    "                if counter == 0:\n",
    "                    counter = 1\n",
    "                    modeType = 1\n",
    "        if counter != 0:\n",
    "    \n",
    "            if counter == 1:\n",
    "                #getting the data\n",
    "    \n",
    "                studentInfo = db.reference(f'students/{id}').get()\n",
    "                print(studentInfo)\n",
    "                # get image from the storage\n",
    "                blob=bucket.get_blob(f'images/{id}.png')\n",
    "                array= np.frombuffer(blob.download_as_string(), np.uint8)\n",
    "                imgStudent = cv2.imdecode(array, cv2.IMREAD_COLOR)\n",
    "                \n",
    "                #update data of voting\n",
    "                datetimeObject = datetime.strptime(studentInfo['last_attendance_time'],\n",
    "                                                   \"%Y-%m-%d %H:%M:%S\")\n",
    "                secondsElapsed = (datetime.now() - datetimeObject).total_seconds()\n",
    "                print(secondsElapsed)\n",
    "                if secondsElapsed > 30:\n",
    "                    ref = db.reference(f'Students/{id}')\n",
    "                    studentInfo['total_attendance'] += 1\n",
    "                    ref.child('total_attendance').set(studentInfo['total_attendance'])\n",
    "                    ref.child('last_attendance_time').set(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                else:\n",
    "                    modeType = 3\n",
    "                    counter = 0\n",
    "                    imgBackground[44:44 + imgModeList[modeType].shape[0], 808:808 + imgModeList[modeType].shape[1]] = imgModeList[modeType]\n",
    "\n",
    "    \n",
    "                # updating data into server\n",
    "                ref.child('total_attendance').set(studentInfo['total_attendance'])\n",
    "            \n",
    "            if modeType != 3:\n",
    "\n",
    "                if 10 < counter < 20:\n",
    "                    modeType = 2\n",
    "\n",
    "            imgBackground[44:44 + imgModeList[modeType].shape[0], 808:808 + imgModeList[modeType].shape[1]] = imgModeList[modeType]\n",
    "\n",
    "            if counter <= 10:\n",
    "                cv2.putText(imgBackground, str(studentInfo['total attendance']), (861,125),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 1, (255,255,255),1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['major']), (1006,550),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.5, (255,255,255),1)\n",
    "                cv2.putText(imgBackground, str(id), (1006, 493),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.5, (255,255,255),1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['standing']), (910, 625),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.6, (100,100,100), 1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['year']), (1025,625),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.6, (100,100,100),1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['starting year']), (1125,625),\n",
    "                             cv2.FONT_HERSHEY_COMPLEX, 0.6, (100,100,100),1)\n",
    "                \n",
    "        \n",
    "        \n",
    "                (w,h), _=cv2.getTextSize(stundentInfo['name'],cv2.FONT_HERSHEY_COMPLEX,  1, 1)\n",
    "                offset=(414 -w)//2\n",
    "                cv2.putText(imgBackground, str(studentInfo['name']),(808+offset, 445),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 1, (50,50,50),1)\n",
    "        \n",
    "                imgBackground[175:175+216,909:909+216]=imgStudent\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            if counter >= 20:\n",
    "                counter = 0\n",
    "                modeType = 0\n",
    "                studentInfo = []\n",
    "                imgStudent = []\n",
    "                imgBackground[44:44 + imgModeList[modeType].shape[0], 808:808 + imgModeList[modeType].shape[1]] = imgModeList[modeType]\n",
    " \n",
    "    else:\n",
    "         modeType = 0\n",
    "         counter = 0\n",
    "        \n",
    "        \n",
    "    #cv2.imshow(\"webcam\", img)\n",
    "    cv2.imshow(\"face identification\", imgBackground)    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53df5206-0958-4e5b-8bef-43d183c34e24",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The default Firebase app does not exist. Make sure to initialize the SDK by calling initialize_app().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfirebase_admin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m storage\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Initialize Firebase Storage bucket\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m bucket \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Initialize Firebase app\u001b[39;00m\n\u001b[0;32m     18\u001b[0m cred \u001b[38;5;241m=\u001b[39m credentials\u001b[38;5;241m.\u001b[39mCertificate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124muser\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mvotingService key.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\firebase_admin\\storage.py:51\u001b[0m, in \u001b[0;36mbucket\u001b[1;34m(name, app)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbucket\u001b[39m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, app\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m storage\u001b[38;5;241m.\u001b[39mBucket:\n\u001b[0;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a handle to a Google Cloud Storage bucket.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    If the name argument is not provided, uses the 'storageBucket' option specified when\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m          or if the specified bucket name is not a valid string.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     client \u001b[38;5;241m=\u001b[39m \u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_app_service\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_STORAGE_ATTRIBUTE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_StorageClient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_app\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m client\u001b[38;5;241m.\u001b[39mbucket(name)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\firebase_admin\\_utils.py:97\u001b[0m, in \u001b[0;36mget_app_service\u001b[1;34m(app, name, initializer)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_app_service\u001b[39m(app, name, initializer):\n\u001b[1;32m---> 97\u001b[0m     app \u001b[38;5;241m=\u001b[39m \u001b[43m_get_initialized_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m app\u001b[38;5;241m.\u001b[39m_get_service(name, initializer)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\firebase_admin\\_utils.py:82\u001b[0m, in \u001b[0;36m_get_initialized_app\u001b[1;34m(app)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a reference to an initialized App instance.\"\"\"\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m app \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfirebase_admin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(app, firebase_admin\u001b[38;5;241m.\u001b[39mApp):\n\u001b[0;32m     85\u001b[0m     initialized_app \u001b[38;5;241m=\u001b[39m firebase_admin\u001b[38;5;241m.\u001b[39mget_app(app\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\firebase_admin\\__init__.py:137\u001b[0m, in \u001b[0;36mget_app\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _apps[name]\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m _DEFAULT_APP_NAME:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe default Firebase app does not exist. Make sure to initialize \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe SDK by calling initialize_app().\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    142\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFirebase app named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m does not exist. Make sure to initialize \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    143\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe SDK by calling initialize_app() with your app name as the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    144\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecond argument.\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(name))\n",
      "\u001b[1;31mValueError\u001b[0m: The default Firebase app does not exist. Make sure to initialize the SDK by calling initialize_app()."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import cvzone\n",
    "import pickle\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, initialize_app, storage, db\n",
    "from datetime import datetime\n",
    "\n",
    "from firebase_admin import storage\n",
    "\n",
    "# Initialize Firebase Storage bucket\n",
    "bucket = storage.bucket()\n",
    "\n",
    "\n",
    "# Initialize Firebase app\n",
    "cred = credentials.Certificate(\"C:\\\\Users\\\\user\\\\Downloads\\\\votingService key.json\")\n",
    "\n",
    "\n",
    "try:\n",
    "    firebase_admin.initialize_app(cred, {\n",
    "        \"databaseURL\": \"https://votingfacerecognition-default-rtdb.firebaseio.com/\",\n",
    "        \"storageBucket\": 'votingfacerecognition.appspot.com'\n",
    "    })\n",
    "except Exception as e:\n",
    "    print(\"Firebase initialization error:\", e)\n",
    "\n",
    "\n",
    "# Initialize database\n",
    "db = firebase_admin.db\n",
    "\n",
    "cap = cv2.VideoCapture(1)  # Change camera index as needed\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Couldn't open camera\")\n",
    "    exit()\n",
    "\n",
    "# cap.set(3, 640)  # Width=640\n",
    "# cap.set(4, 480)  # Height=480\n",
    "\n",
    "imgBackground = cv2.imread(r'C:\\Users\\user\\Desktop\\face recogination real db\\resources\\background.png')\n",
    "\n",
    "if imgBackground is None:\n",
    "    print(\"Couldn't load background image\")\n",
    "    exit()\n",
    "\n",
    "folderModePath = r\"C:\\Users\\user\\Desktop\\face recogination real db\\resources\\modes\"\n",
    "ModePath = os.listdir(folderModePath)\n",
    "imgModeList = []\n",
    "encodeListKnown = []\n",
    "studentId = []\n",
    "\n",
    "for path in ModePath:\n",
    "    img_mode = cv2.imread(os.path.join(folderModePath, path))\n",
    "    if img_mode is None:\n",
    "        print(f\"Couldn't load image: {path}\")\n",
    "        continue\n",
    "    imgModeList.append(img_mode)\n",
    "\n",
    "    # Encode the face in the current image and append to encodeListKnown\n",
    "    face_encodings = face_recognition.face_encodings(img_mode)\n",
    "    if len(face_encodings) > 0:\n",
    "        encodeListKnown.append(face_encodings[0])  # Assuming there's only one face per image\n",
    "        studentId.append(path.split('.')[0])  # Extract student ID from the filename\n",
    "\n",
    "print(\"Number of loaded images:\", len(imgModeList))\n",
    "\n",
    "print(\"loading the encoding file\")\n",
    "with open('encodeFile.p', 'rb') as file:\n",
    "    encodeListKnownWithIds = pickle.load(file)  # load hui file now extract\n",
    "\n",
    "encodeListKnown.extend(encodeListKnownWithIds[0])  # Append previously loaded encodings\n",
    "studentId.extend(encodeListKnownWithIds[1])  # Append previously loaded student IDs\n",
    "print(\"the encoding file loaded\")\n",
    "\n",
    "\n",
    "modeType = 0\n",
    "counter = 0\n",
    "id = -1\n",
    "imgStudent = []\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        print(\"Couldn't read frame\")\n",
    "        break\n",
    "\n",
    "    imgSmall = cv2.resize(img, (0, 0), None, 0.25, 0.25)  # (0,0)-->pixel size 0.25,0.25 --> koi b size ho img ka use 1/4th krdo\n",
    "    imgSmall = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faceCurrFrame = face_recognition.face_locations(imgSmall)\n",
    "    encodeCurrFrame = face_recognition.face_encodings(imgSmall, faceCurrFrame)  # sare faces ki encoding ni krni curently available face ki krni h -->given faceCurrFrame\n",
    "   \n",
    "    imgBackground[162:162 + 480, 55:55 + 640] = img\n",
    "    imgBackground[44:44 + imgModeList[modeType].shape[0], 808:808 + imgModeList[modeType].shape[1]] = imgModeList[modeType]\n",
    "\n",
    "    if faceCurrFrame:    \n",
    "        for encodeFace, faceLoc in zip(encodeCurrFrame, faceCurrFrame):\n",
    "            matches = face_recognition.compare_faces(encodeListKnown, encodeFace)  # current face-->encodeFace mathces m compared face store hoga from list of known faces\n",
    "            faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)\n",
    "    \n",
    "            # Get the index of the closest match\n",
    "            matchIndex = np.argmin(faceDis)\n",
    "    \n",
    "            if matches[matchIndex]:\n",
    "                # provinding bounding box information from facelocation --->y1, x2, y2, x1 \n",
    "                y1, x2, y2, x1 = faceLoc     \n",
    "                y1, x2, y2, x1 = y1*4, x2*4, y2*4, x1*4       # *4 coz we reduced size from 1/4th \n",
    "                bbox = 55+x1, 162+y1, x2-x1, y2-y1    # bounding box -->image set hojaegi \n",
    "                imgBackground = cvzone.cornerRect(imgBackground,bbox,rt=0)  # detected image pr box aage--->boundarybox-->rt rectangleWidth\n",
    "    \n",
    "                id = studentId[matchIndex]\n",
    "                if counter == 0:\n",
    "                    counter = 1\n",
    "                    modeType = 1\n",
    "        if counter != 0:\n",
    "            if counter == 1:\n",
    "                # Get student information from the database\n",
    "                studentInfo = db.reference(f'students/{id}').get()\n",
    "                print(studentInfo)\n",
    "                \n",
    "                # Get image from the storage\n",
    "                blob = bucket.get_blob(f'images/{id}.png')\n",
    "                array = np.frombuffer(blob.download_as_string(), np.uint8)\n",
    "                imgStudent = cv2.imdecode(array, cv2.IMREAD_COLOR)\n",
    "                \n",
    "                # Update data of voting\n",
    "                datetimeObject = datetime.strptime(studentInfo['last_attendance_time'], \"%Y-%m-%d %H:%M:%S\")\n",
    "                secondsElapsed = (datetime.now() - datetimeObject).total_seconds()\n",
    "                print(secondsElapsed)\n",
    "                if secondsElapsed > 30:\n",
    "                    ref = db.reference(f'Students/{id}')\n",
    "                    studentInfo['total_attendance'] += 1\n",
    "                    ref.child('total_attendance').set(studentInfo['total_attendance'])\n",
    "                    ref.child('last_attendance_time').set(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                else:\n",
    "                    modeType = 3\n",
    "                    counter = 0\n",
    "                    imgBackground[44:44 + imgModeList[modeType].shape[0], 808:808 + imgModeList[modeType].shape[1]] = imgModeList[modeType]\n",
    "\n",
    "                # Updating data into server\n",
    "                ref.child('total_attendance').set(studentInfo['total_attendance'])\n",
    "            \n",
    "            if modeType != 3:\n",
    "                if 10 < counter < 20:\n",
    "                    modeType = 2\n",
    "\n",
    "            imgBackground[44:44 + imgModeList[modeType].shape[0], 808:808 + imgModeList[modeType].shape[1]] = imgModeList[modeType]\n",
    "\n",
    "            if 10<counter<20:\n",
    "                modeType=2\n",
    "\n",
    "\n",
    "            imgBackground[44:44+633,800:800+414]==imgModeList[modeType]\n",
    "\n",
    "            \n",
    "            if counter <= 10:\n",
    "                cv2.putText(imgBackground, str(studentInfo['total attendance']), (861, 125),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['major']), (1006, 550),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                cv2.putText(imgBackground, str(id), (1006, 493),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['standing']), (910, 625),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.6, (100, 100, 100), 1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['year']), (1025, 625),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.6, (100, 100, 100), 1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['starting year']), (1125, 625),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.6, (100, 100, 100), 1)\n",
    "                \n",
    "                (w, h), _ = cv2.getTextSize(studentInfo['name'], cv2.FONT_HERSHEY_COMPLEX, 1, 1)\n",
    "                offset = (414 - w) // 2\n",
    "                cv2.putText(imgBackground, str(studentInfo['name']), (808 + offset, 445),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 1, (50, 50, 50), 1)\n",
    "        \n",
    "                imgBackground[175:175 + 216, 909:909 + 216] = imgStudent\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            if counter >= 20:\n",
    "                counter = 0\n",
    "                modeType = 0\n",
    "                studentInfo = []\n",
    "                imgStudent = []\n",
    "                imgBackground[44:44 + imgModeList[modeType].shape[0], 808:808 + imgModeList[modeType].shape[1]] = imgModeList[modeType]\n",
    " \n",
    "    else:\n",
    "        modeType = 0\n",
    "        counter = 0\n",
    "        \n",
    "    cv2.imshow(\"face identification\", imgBackground)    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a80b5ad8-992c-4646-a5ae-7ef841cefcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded images: 4\n",
      "Loading saved encodings...\n",
      "Saved encodings loaded\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The default Firebase app does not exist. Make sure to initialize the SDK by calling initialize_app().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 112\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m counter \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m counter \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    111\u001b[0m         \u001b[38;5;66;03m# Get student information from the database\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m         studentInfo \u001b[38;5;241m=\u001b[39m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstudents/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mid\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget()\n\u001b[0;32m    113\u001b[0m         \u001b[38;5;28mprint\u001b[39m(studentInfo)\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;66;03m# Get image from the storage\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\firebase_admin\\db.py:68\u001b[0m, in \u001b[0;36mreference\u001b[1;34m(path, app, url)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreference\u001b[39m(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, app\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, url\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     49\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a database ``Reference`` representing the node at the specified path.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    If no path is specified, this function returns a ``Reference`` that represents the database\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m      ValueError: If the specified path or app is invalid.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     service \u001b[38;5;241m=\u001b[39m \u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_app_service\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_DB_ATTRIBUTE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_DatabaseService\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     client \u001b[38;5;241m=\u001b[39m service\u001b[38;5;241m.\u001b[39mget_client(url)\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Reference(client\u001b[38;5;241m=\u001b[39mclient, path\u001b[38;5;241m=\u001b[39mpath)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\firebase_admin\\_utils.py:97\u001b[0m, in \u001b[0;36mget_app_service\u001b[1;34m(app, name, initializer)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_app_service\u001b[39m(app, name, initializer):\n\u001b[1;32m---> 97\u001b[0m     app \u001b[38;5;241m=\u001b[39m \u001b[43m_get_initialized_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m app\u001b[38;5;241m.\u001b[39m_get_service(name, initializer)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\firebase_admin\\_utils.py:82\u001b[0m, in \u001b[0;36m_get_initialized_app\u001b[1;34m(app)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns a reference to an initialized App instance.\"\"\"\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m app \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfirebase_admin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(app, firebase_admin\u001b[38;5;241m.\u001b[39mApp):\n\u001b[0;32m     85\u001b[0m     initialized_app \u001b[38;5;241m=\u001b[39m firebase_admin\u001b[38;5;241m.\u001b[39mget_app(app\u001b[38;5;241m.\u001b[39mname)\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\firebase_admin\\__init__.py:137\u001b[0m, in \u001b[0;36mget_app\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _apps[name]\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m _DEFAULT_APP_NAME:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    138\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe default Firebase app does not exist. Make sure to initialize \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe SDK by calling initialize_app().\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    142\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFirebase app named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m does not exist. Make sure to initialize \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    143\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe SDK by calling initialize_app() with your app name as the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    144\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecond argument.\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(name))\n",
      "\u001b[1;31mValueError\u001b[0m: The default Firebase app does not exist. Make sure to initialize the SDK by calling initialize_app()."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import cvzone\n",
    "import pickle\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, initialize_app, storage, db\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Firebase app\n",
    "try:\n",
    "    firebase_admin.get_app(name='my_app')\n",
    "except ValueError:\n",
    "    cred = credentials.Certificate(\"C:\\\\Users\\\\user\\\\Downloads\\\\votingService key.json\")\n",
    "    firebase_admin.initialize_app(cred, {\n",
    "        \"databaseURL\": \"https://votingfacerecognition-default-rtdb.firebaseio.com/\",\n",
    "        \"storageBucket\": 'votingfacerecognition.appspot.com'\n",
    "    }, name='my_app')\n",
    "\n",
    "# Initialize Firebase Storage bucket\n",
    "bucket = storage.bucket(app=firebase_admin.get_app(name='my_app'))\n",
    "\n",
    "# Initialize database\n",
    "db = firebase_admin.db\n",
    "\n",
    "cap = cv2.VideoCapture(1)  # Change camera index as needed\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Couldn't open camera\")\n",
    "    exit()\n",
    "\n",
    "# Load background image\n",
    "imgBackground = cv2.imread(r'C:\\Users\\user\\Desktop\\face recogination real db\\resources\\background.png')\n",
    "\n",
    "if imgBackground is None:\n",
    "    print(\"Couldn't load background image\")\n",
    "    exit()\n",
    "\n",
    "# Load images for face recognition\n",
    "folderModePath = r\"C:\\Users\\user\\Desktop\\face recogination real db\\resources\\modes\"\n",
    "ModePath = os.listdir(folderModePath)\n",
    "imgModeList = []\n",
    "encodeListKnown = []\n",
    "studentId = []\n",
    "\n",
    "for path in ModePath:\n",
    "    img_mode = cv2.imread(os.path.join(folderModePath, path))\n",
    "    if img_mode is None:\n",
    "        print(f\"Couldn't load image: {path}\")\n",
    "        continue\n",
    "    imgModeList.append(img_mode)\n",
    "\n",
    "    # Encode the face in the current image and append to encodeListKnown\n",
    "    face_encodings = face_recognition.face_encodings(img_mode)\n",
    "    if len(face_encodings) > 0:\n",
    "        encodeListKnown.append(face_encodings[0])  # Assuming there's only one face per image\n",
    "        studentId.append(path.split('.')[0])  # Extract student ID from the filename\n",
    "\n",
    "print(\"Number of loaded images:\", len(imgModeList))\n",
    "\n",
    "# Load saved encodings\n",
    "print(\"Loading saved encodings...\")\n",
    "with open('encodeFile.p', 'rb') as file:\n",
    "    encodeListKnownWithIds = pickle.load(file)  # load hui file now extract\n",
    "\n",
    "encodeListKnown.extend(encodeListKnownWithIds[0])  # Append previously loaded encodings\n",
    "studentId.extend(encodeListKnownWithIds[1])  # Append previously loaded student IDs\n",
    "print(\"Saved encodings loaded\")\n",
    "\n",
    "modeType = 0\n",
    "counter = 0\n",
    "id = -1\n",
    "imgStudent = []\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        print(\"Couldn't read frame\")\n",
    "        break\n",
    "\n",
    "    imgSmall = cv2.resize(img, (0, 0), None, 0.25, 0.25)  # (0,0)-->pixel size 0.25,0.25 --> koi b size ho img ka use 1/4th krdo\n",
    "    imgSmall = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faceCurrFrame = face_recognition.face_locations(imgSmall)\n",
    "    encodeCurrFrame = face_recognition.face_encodings(imgSmall, faceCurrFrame)  # sare faces ki encoding ni krni curently available face ki krni h -->given faceCurrFrame\n",
    "   \n",
    "    imgBackground[162:162 + 480, 55:55 + 640] = img\n",
    "    imgBackground[44:44 + imgModeList[modeType].shape[0], 808:808 + imgModeList[modeType].shape[1]] = imgModeList[modeType]\n",
    "\n",
    "    if faceCurrFrame:    \n",
    "        for encodeFace, faceLoc in zip(encodeCurrFrame, faceCurrFrame):\n",
    "            matches = face_recognition.compare_faces(encodeListKnown, encodeFace)  # current face-->encodeFace mathces m compared face store hoga from list of known faces\n",
    "            faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)\n",
    "    \n",
    "            # Get the index of the closest match\n",
    "            matchIndex = np.argmin(faceDis)\n",
    "    \n",
    "            if matches[matchIndex]:\n",
    "                # provinding bounding box information from facelocation --->y1, x2, y2, x1 \n",
    "                y1, x2, y2, x1 = faceLoc     \n",
    "                y1, x2, y2, x1 = y1*4, x2*4, y2*4, x1*4       # *4 coz we reduced size from 1/4th \n",
    "                bbox = 55+x1, 162+y1, x2-x1, y2-y1    # bounding box -->image set hojaegi \n",
    "                imgBackground = cvzone.cornerRect(imgBackground,bbox,rt=0)  # detected image pr box aage--->boundarybox-->rt rectangleWidth\n",
    "    \n",
    "                id = studentId[matchIndex]\n",
    "                if counter == 0:\n",
    "                    counter = 1\n",
    "                    modeType = 1\n",
    "        if counter != 0:\n",
    "            if counter == 1:\n",
    "                # Get student information from the database\n",
    "                studentInfo = db.reference(f'students/{id}').get()\n",
    "                print(studentInfo)\n",
    "                \n",
    "                # Get image from the storage\n",
    "                blob = bucket.get_blob(f'images/{id}.png')\n",
    "                array = np.frombuffer(blob.download_as_string(), np.uint8)\n",
    "                imgStudent = cv2.imdecode(array, cv2.IMREAD_COLOR)\n",
    "                \n",
    "                # Update data of voting\n",
    "                datetimeObject = datetime.strptime(studentInfo['last_attendance_time'], \"%Y-%m-%d %H:%M:%S\")\n",
    "                secondsElapsed = (datetime.now() - datetimeObject).total_seconds()\n",
    "                print(secondsElapsed)\n",
    "                if secondsElapsed > 30:\n",
    "                    ref = db.reference(f'Students/{id}')\n",
    "                    studentInfo['total_attendance'] += 1\n",
    "                    ref.child('total_attendance').set(studentInfo['total_attendance'])\n",
    "                    ref.child('last_attendance_time').set(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                else:\n",
    "                    modeType = 3\n",
    "                    counter = 0\n",
    "                    imgBackground[44:44 + imgModeList[modeType].shape[0], 808:808 + imgModeList[modeType].shape[1]] = imgModeList[modeType]\n",
    "\n",
    "                # Updating data into server\n",
    "                ref.child('total_attendance').set(studentInfo['total_attendance'])\n",
    "            \n",
    "            if modeType != 3:\n",
    "                if 10 < counter < 20:\n",
    "                    modeType = 2\n",
    "\n",
    "            imgBackground[44:44 + imgModeList[modeType].shape[0], 808:808 + imgModeList[modeType].shape[1]] = imgModeList[modeType]\n",
    "\n",
    "            if 10<counter<20:\n",
    "                modeType=2\n",
    "\n",
    "\n",
    "            imgBackground[44:44+633,800:800+414]==imgModeList[modeType]\n",
    "\n",
    "            \n",
    "            if counter <= 10:\n",
    "                cv2.putText(imgBackground, str(studentInfo['total attendance']), (861, 125),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['major']), (1006, 550),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                cv2.putText(imgBackground, str(id), (1006, 493),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['standing']), (910, 625),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.6, (100, 100, 100), 1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['year']), (1025, 625),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.6, (100, 100, 100), 1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['starting year']), (1125, 625),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.6, (100, 100, 100), 1)\n",
    "                \n",
    "                (w, h), _ = cv2.getTextSize(studentInfo['name'], cv2.FONT_HERSHEY_COMPLEX, 1, 1)\n",
    "                offset = (414 - w) // 2\n",
    "                cv2.putText(imgBackground, str(studentInfo['name']), (808 + offset, 445),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 1, (50, 50, 50), 1)\n",
    "        \n",
    "                imgBackground[175:175 + 216, 909:909 + 216] = imgStudent\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            if counter >= 20:\n",
    "                counter = 0\n",
    "                modeType = 0\n",
    "                studentInfo = []\n",
    "                imgStudent = []\n",
    "                imgBackground[44:44 + imgModeList[modeType].shape[0], 808:808 + imgModeList[modeType].shape[1]] = imgModeList[modeType]\n",
    " \n",
    "    else:\n",
    "        modeType = 0\n",
    "        counter = 0\n",
    "        \n",
    "    cv2.imshow(\"face identification\", imgBackground)    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72ea8d50-b9ad-4856-8185-4ab39e715c49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded images: 4\n",
      "loading the encoding file\n",
      "the encoding file loaded\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (480,640,3) into shape (633,414,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 92\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m#imgBackground[162:162 + 480, 55:55 + 640] = img\u001b[39;00m\n\u001b[0;32m     91\u001b[0m imgBackground[\u001b[38;5;241m162\u001b[39m:\u001b[38;5;241m162\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m480\u001b[39m, \u001b[38;5;241m55\u001b[39m:\u001b[38;5;241m55\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m640\u001b[39m] \u001b[38;5;241m=\u001b[39m img\n\u001b[1;32m---> 92\u001b[0m \u001b[43mimgBackground\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m44\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m44\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m633\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m808\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m808\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m414\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m imgModeList[modeType]\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# Resize the image from imgModeList[1] to match the dimensions\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m#img_mode_resized = cv2.resize(imgModeList[1], (414, 633))\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    102\u001b[0m \n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m#imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]  # Assign the selected image to the background\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m faceCurrFrame:    \n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (480,640,3) into shape (633,414,3)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import cvzone\n",
    "import pickle\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "from firebase_admin import credentials, initialize_app, storage, db\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize Firebase app\n",
    "cred = credentials.Certificate(\"C:\\\\Users\\\\user\\\\Downloads\\\\votingService key.json\")\n",
    "\n",
    "try:\n",
    "    firebase_admin.initialize_app(cred, {\n",
    "        \"databaseURL\": \"https://votingfacerecognition-default-rtdb.firebaseio.com/\",\n",
    "        \"storageBucket\": 'votingfacerecognition.appspot.com'\n",
    "    })\n",
    "except ValueError:\n",
    "    pass  # Firebase app is already initialized\n",
    "\n",
    "# Check if Firebase app is already initialized\n",
    "try:\n",
    "    firebase_admin.get_app()\n",
    "except ValueError as e:\n",
    "    print(\"Firebase initialization error:\", e)\n",
    "    exit()\n",
    "\n",
    "cap = cv2.VideoCapture(1)  # Change camera index as needed\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Couldn't open camera\")\n",
    "    exit()\n",
    "\n",
    "cap.set(3, 640)  # Width=640\n",
    "cap.set(4, 480)  # Height=480\n",
    "\n",
    "imgBackground = cv2.imread(r'C:\\Users\\user\\Desktop\\face recogination real db\\resources\\background.png')\n",
    "\n",
    "if imgBackground is None:\n",
    "    print(\"Couldn't load background image\")\n",
    "    exit()\n",
    "\n",
    "folderModePath = r\"C:\\Users\\user\\Desktop\\face recogination real db\\resources\\modes\"\n",
    "ModePath = os.listdir(folderModePath)\n",
    "imgModeList = []\n",
    "encodeListKnown = []\n",
    "studentId = []\n",
    "\n",
    "for path in ModePath:\n",
    "    img_mode = cv2.imread(os.path.join(folderModePath, path))\n",
    "    if img_mode is None:\n",
    "        print(f\"Couldn't load image: {path}\")\n",
    "        continue\n",
    "    img_mode_resized = cv2.resize(img_mode, (640, 480))\n",
    "    imgModeList.append(img_mode_resized)\n",
    "\n",
    "    # Encode the face in the current image and append to encodeListKnown\n",
    "    face_encodings = face_recognition.face_encodings(img_mode_resized)\n",
    "    if len(face_encodings) > 0:\n",
    "        encodeListKnown.append(face_encodings[0])  # Assuming there's only one face per image\n",
    "        studentId.append(path.split('.')[0])  # Extract student ID from the filename\n",
    "\n",
    "print(\"Number of loaded images:\", len(imgModeList))\n",
    "\n",
    "print(\"loading the encoding file\")\n",
    "file = open('encodeFile.p', 'rb')\n",
    "encodeListKnownWithIds = pickle.load(file)  # load hui file now extract\n",
    "file.close()\n",
    "encodeListKnown.extend(encodeListKnownWithIds[0])  # Append previously loaded encodings\n",
    "studentId.extend(encodeListKnownWithIds[1])  # Append previously loaded student IDs\n",
    "print(\"the encoding file loaded\")\n",
    "\n",
    "\n",
    "modeType = 0\n",
    "counter=0\n",
    "id = -1\n",
    "imgStudent = []\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        print(\"Couldn't read frame\")\n",
    "        break\n",
    "\n",
    "    imgSmall = cv2.resize(img, (0, 0), None, 0.25, 0.25)  # (0,0)-->pixel size 0.25,0.25 --> koi b size ho img ka use 1/4th krdo\n",
    "    imgSmall = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faceCurrFrame = face_recognition.face_locations(imgSmall)\n",
    "    encodeCurrFrame = face_recognition.face_encodings(imgSmall, faceCurrFrame)  # sare faces ki encoding ni krni curently available face ki krni h -->given faceCurrFrame\n",
    "   \n",
    "    #imgBackground[162:162 + 480, 55:55 + 640] = img\n",
    "    imgBackground[162:162 + 480, 55:55 + 640] = img\n",
    "    imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    "\n",
    "   \n",
    "    # Resize the image from imgModeList[1] to match the dimensions\n",
    "    #img_mode_resized = cv2.resize(imgModeList[1], (414, 633))\n",
    "    \n",
    "    #imgBackground[44:44 + 633, 808:808 + 414] =img_mode_resized \n",
    "    #imgModeList[0]= img_mode_resized \n",
    "   # imgBackground[44:44 + 633, 808:808 + 414] = cv2.resize(imgModeList[modeType], (414, 633))\n",
    "\n",
    "\n",
    "    #imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]  # Assign the selected image to the background\n",
    "\n",
    "    if faceCurrFrame:    \n",
    "        for encodeFace, faceLoc in zip(encodeCurrFrame, faceCurrFrame):\n",
    "            matches = face_recognition.compare_faces(encodeListKnown, encodeFace)  # current face-->encodeFace mathces m compared face store hoga from list of known faces\n",
    "            faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)\n",
    "    \n",
    "            for match, distance in zip(matches, faceDis):\n",
    "                print(\"Match:\", match)\n",
    "                print(\"Distance:\", distance)\n",
    "    \n",
    "            # Get the index of the closest match\n",
    "            matchIndex = np.argmin(faceDis)\n",
    "    \n",
    "            if matches[matchIndex]:\n",
    "                #print(\"Known face detected\")\n",
    "                #print(studentId[matchIndex])\n",
    "    \n",
    "                \n",
    "                # provinding bounding box information from facelocation --->y1, x2, y2, x1 \n",
    "                y1, x2, y2, x1 = faceLoc     \n",
    "                y1, x2, y2, x1 =y1*4, x2*4, y2*4, x1*4       # *4 coz we reduced size from 1/4th \n",
    "                bbox = 55+x1, 162+y1, x2-x1, y2-y1    # bounding box -->image set hojaegi \n",
    "                imgBackground = cvzone.cornerRect(imgBackground,bbox,rt=0)  # detected image pr box aage--->boundarybox-->rt rectangleWidth\n",
    "    \n",
    "    \n",
    "                id= studentId[matchIndex]\n",
    "                if counter == 0:\n",
    "                    counter = 1\n",
    "                    modeType = 1\n",
    "        if counter !=0:\n",
    "    \n",
    "            if counter==1:\n",
    "                #getting the data\n",
    "    \n",
    "                studentInfo = db.reference(f'students/{id}').get()\n",
    "                print(studentInfo)\n",
    "                # get image from the storage\n",
    "                blob=bucket.get_blob(f'images/{id}.png')\n",
    "                array= np.frombuffer(blob.download_as_string(), np.uint8)\n",
    "                imgStudent = cv2.imdecode(array,cv2.COLOR_BGRA2BGR)\n",
    "                \n",
    "                #update data of voting\n",
    "                datetimeObject = datetime.strptime(studentInfo['last_attendance_time'],\n",
    "                                                   \"%Y-%m-%d %H:%M:%S\")\n",
    "                secondsElapsed = (datetime.now() - datetimeObject).total_seconds()\n",
    "                print(secondsElapsed)\n",
    "                if secondsElapsed > 30:\n",
    "                    ref = db.reference(f'Students/{id}')\n",
    "                    studentInfo['total_attendance'] += 1\n",
    "                    ref.child('total_attendance').set(studentInfo['total_attendance'])\n",
    "                    ref.child('last_attendance_time').set(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                else:\n",
    "                    modeType = 3\n",
    "                    counter = 0\n",
    "                    imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    "\n",
    "    \n",
    "                # updating data into server\n",
    "                ref.child('total_attendance').set(studentInfo['total_attendance'])\n",
    "            \n",
    "            if modeType != 3:\n",
    "\n",
    "                if 10 < counter < 20:\n",
    "                    modeType = 2\n",
    "\n",
    "            imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    "\n",
    "            if counter <= 10:\n",
    "                cv2.putText(imgBackground, str(studentInfo['total attendance']), (861,125),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 1, (255,255,255),1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['major']), (1006,550),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.5, (255,255,255),1)\n",
    "                cv2.putText(imgBackground, str(id), (1006, 493),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.5, (255,255,255),1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['standing']), (910, 625),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.6, (100,100,100), 1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['year']), (1025,625),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.6, (100,100,100),1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['starting year']), (1125,625),\n",
    "                             cv2.FONT_HERSHEY_COMPLEX, 0.6, (100,100,100),1)\n",
    "                \n",
    "        \n",
    "        \n",
    "                (w,h), _=cv2.getTextSize(stundentInfo['name'],cv2.FONT_HERSHEY_COMPLEX,  1, 1)\n",
    "                offset=(414 -w)//2\n",
    "                cv2.putText(imgBAckground, str(studentInfo['name']),(808+offset, 445),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 1, (50,50,50),1)\n",
    "        \n",
    "                imgBackground[175:175+216,909:909+216]=imgStudent\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            if counter >= 20:\n",
    "                counter = 0\n",
    "                modeType = 0\n",
    "                studentInfo = []\n",
    "                imgStudent = []\n",
    "                imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    " \n",
    "    else:\n",
    "         modeType = 0\n",
    "         counter = 0\n",
    "        \n",
    "        \n",
    "    #cv2.imshow(\"webcam\", img)\n",
    "    cv2.imshow(\"face identification\", imgBackground)    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "    \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22ceb13e-4a9c-4222-af9e-899e9a5bae4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of loaded images: 4\n",
      "loading the encoding file\n",
      "the encoding file loaded\n",
      "{'last_attendance_time': '2022-12-11 00:54:34', 'major': 'Robotics', 'name': 'Murtaza Hassan', 'standing': 'G', 'starting_year': 2017, 'total_attendance': 7, 'year': 4}\n",
      "Error: Blob 'images/1.png' not found in Firebase Storage.\n",
      "38120981.070699\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'total attendance'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 148\u001b[0m\n\u001b[0;32m    145\u001b[0m imgBackground[\u001b[38;5;241m44\u001b[39m:\u001b[38;5;241m44\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m633\u001b[39m, \u001b[38;5;241m808\u001b[39m:\u001b[38;5;241m808\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m414\u001b[39m] \u001b[38;5;241m=\u001b[39m imgModeList[modeType]\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m counter \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m--> 148\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(imgBackground, \u001b[38;5;28mstr\u001b[39m(\u001b[43mstudentInfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal attendance\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m), (\u001b[38;5;241m861\u001b[39m, \u001b[38;5;241m125\u001b[39m),\n\u001b[0;32m    149\u001b[0m                 cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_COMPLEX, \u001b[38;5;241m1\u001b[39m, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    150\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(imgBackground, \u001b[38;5;28mstr\u001b[39m(studentInfo[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmajor\u001b[39m\u001b[38;5;124m'\u001b[39m]), (\u001b[38;5;241m1006\u001b[39m, \u001b[38;5;241m550\u001b[39m),\n\u001b[0;32m    151\u001b[0m                 cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_COMPLEX, \u001b[38;5;241m0.5\u001b[39m, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    152\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mputText(imgBackground, \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m), (\u001b[38;5;241m1006\u001b[39m, \u001b[38;5;241m493\u001b[39m),\n\u001b[0;32m    153\u001b[0m                 cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_COMPLEX, \u001b[38;5;241m0.5\u001b[39m, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'total attendance'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import cvzone\n",
    "import pickle\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import firebase_admin\n",
    "from firebase_admin import credentials, initialize_app, storage, db\n",
    "\n",
    "# Check if Firebase app is already initialized\n",
    "try:\n",
    "    firebase_admin.get_app(name='my_app')\n",
    "except ValueError as e:\n",
    "    # If Firebase app is not initialized, initialize it with a unique name\n",
    "    cred = credentials.Certificate(\"C:\\\\Users\\\\user\\\\Downloads\\\\votingService key.json\")\n",
    "    initialize_app(cred, {\n",
    "        \"databaseURL\": \"https://votingfacerecognition-default-rtdb.firebaseio.com/\",\n",
    "        \"storageBucket\": 'votingfacerecognition.appspot.com'\n",
    "    }, name='my_app')\n",
    "\n",
    "# Initialize the database\n",
    "db = firebase_admin.db\n",
    "\n",
    "cap = cv2.VideoCapture(1)  # Change camera index as needed\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"Couldn't open camera\")\n",
    "    exit()\n",
    "\n",
    "cap.set(3, 640)  # Width=640\n",
    "cap.set(4, 480)  # Height=480\n",
    "\n",
    "imgBackground = cv2.imread(r'C:\\Users\\user\\Desktop\\face recogination real db\\resources\\background.png')\n",
    "\n",
    "if imgBackground is None:\n",
    "    print(\"Couldn't load background image\")\n",
    "    exit()\n",
    "\n",
    "folderModePath = r\"C:\\Users\\user\\Desktop\\face recogination real db\\resources\\modes\"\n",
    "ModePath = os.listdir(folderModePath)\n",
    "imgModeList = []\n",
    "encodeListKnown = []\n",
    "studentId = []\n",
    "\n",
    "for path in ModePath:\n",
    "    img_mode = cv2.imread(os.path.join(folderModePath, path))\n",
    "    if img_mode is None:\n",
    "        print(f\"Couldn't load image: {path}\")\n",
    "        continue\n",
    "    img_mode_resized = cv2.resize(img_mode, (414, 633))  # Resize image to match the size of the region\n",
    "    imgModeList.append(img_mode_resized)\n",
    "\n",
    "    # Encode the face in the current image and append to encodeListKnown\n",
    "    face_encodings = face_recognition.face_encodings(img_mode_resized)\n",
    "    if len(face_encodings) > 0:\n",
    "        encodeListKnown.append(face_encodings[0])  # Assuming there's only one face per image\n",
    "        studentId.append(path.split('.')[0])  # Extract student ID from the filename\n",
    "\n",
    "print(\"Number of loaded images:\", len(imgModeList))\n",
    "\n",
    "print(\"loading the encoding file\")\n",
    "file = open('encodeFile.p', 'rb')\n",
    "encodeListKnownWithIds = pickle.load(file)  # load hui file now extract\n",
    "file.close()\n",
    "encodeListKnown.extend(encodeListKnownWithIds[0])  # Append previously loaded encodings\n",
    "studentId.extend(encodeListKnownWithIds[1])  # Append previously loaded student IDs\n",
    "print(\"the encoding file loaded\")\n",
    "\n",
    "modeType = 0\n",
    "counter = 0\n",
    "id = -1\n",
    "imgStudent = []\n",
    "\n",
    "while True:\n",
    "    success, img = cap.read()\n",
    "    if not success:\n",
    "        print(\"Couldn't read frame\")\n",
    "        break\n",
    "\n",
    "    imgSmall = cv2.resize(img, (0, 0), None, 0.25, 0.25)  # (0,0)-->pixel size 0.25,0.25 --> koi b size ho img ka use 1/4th krdo\n",
    "    imgSmall = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    faceCurrFrame = face_recognition.face_locations(imgSmall)\n",
    "    encodeCurrFrame = face_recognition.face_encodings(imgSmall, faceCurrFrame)  # sare faces ki encoding ni krni curently available face ki krni h -->given faceCurrFrame\n",
    "   \n",
    "    imgBackground[162:162 + 480, 55:55 + 640] = img\n",
    "    imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]  # Assign the selected image to the background\n",
    "\n",
    "    if faceCurrFrame:    \n",
    "        for encodeFace, faceLoc in zip(encodeCurrFrame, faceCurrFrame):\n",
    "            matches = face_recognition.compare_faces(encodeListKnown, encodeFace)  # current face-->encodeFace mathces m compared face store hoga from list of known faces\n",
    "            faceDis = face_recognition.face_distance(encodeListKnown, encodeFace)\n",
    "    \n",
    "            # Get the index of the closest match\n",
    "            matchIndex = np.argmin(faceDis)\n",
    "    \n",
    "            if matches[matchIndex]:\n",
    "                # provinding bounding box information from facelocation --->y1, x2, y2, x1 \n",
    "                y1, x2, y2, x1 = faceLoc     \n",
    "                y1, x2, y2, x1 = y1 * 4, x2 * 4, y2 * 4, x1 * 4  # *4 coz we reduced size from 1/4th \n",
    "                bbox = 55 + x1, 162 + y1, x2 - x1, y2 - y1  # bounding box -->image set hojaegi \n",
    "                imgBackground = cvzone.cornerRect(imgBackground, bbox, rt=0)  # detected image pr box aage--->boundarybox-->rt rectangleWidth\n",
    "    \n",
    "                id = studentId[matchIndex]\n",
    "                if counter == 0:\n",
    "                    counter = 1\n",
    "                    modeType = 1\n",
    "        if counter != 0:\n",
    "            if counter == 1:\n",
    "                # getting the data\n",
    "                studentInfo = db.reference(f'students/{id}').get()\n",
    "                print(studentInfo)\n",
    "                # get image from the storage\n",
    "                # get image from the storage\n",
    "                blob = storage.bucket().get_blob(f'images/{id}.png')\n",
    "                if blob is None:\n",
    "                    print(f\"Error: Blob 'images/{id}.png' not found in Firebase Storage.\")\n",
    "                else:\n",
    "                    array = np.frombuffer(blob.download_as_string(), np.uint8)\n",
    "                    imgStudent = cv2.imdecode(array, cv2.IMREAD_COLOR)\n",
    "\n",
    "                # update data of voting\n",
    "                datetimeObject = datetime.strptime(studentInfo['last_attendance_time'], \"%Y-%m-%d %H:%M:%S\")\n",
    "                secondsElapsed = (datetime.now() - datetimeObject).total_seconds()\n",
    "                print(secondsElapsed)\n",
    "                if secondsElapsed > 30:\n",
    "                    ref = db.reference(f'Students/{id}')\n",
    "                    studentInfo['total_attendance'] += 1\n",
    "                    ref.child('total_attendance').set(studentInfo['total_attendance'])\n",
    "                    ref.child('last_attendance_time').set(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "                else:\n",
    "                    modeType = 3\n",
    "                    counter = 0\n",
    "                    imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    "    \n",
    "                # updating data into server\n",
    "                ref.child('total_attendance').set(studentInfo['total_attendance'])\n",
    "            \n",
    "            if modeType != 3:\n",
    "                if 10 < counter < 20:\n",
    "                    modeType = 2\n",
    "\n",
    "            imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    "\n",
    "            if counter <= 10:\n",
    "                cv2.putText(imgBackground, str(studentInfo['total attendance']), (861, 125),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 1, (255, 255, 255), 1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['major']), (1006, 550),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                cv2.putText(imgBackground, str(id), (1006, 493),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.5, (255, 255, 255), 1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['standing']), (910, 625),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.6, (100, 100, 100), 1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['year']), (1025, 625),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.6, (100, 100, 100), 1)\n",
    "                cv2.putText(imgBackground, str(studentInfo['starting year']), (1125, 625),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 0.6, (100, 100, 100), 1)\n",
    "                \n",
    "                (w, h), _ = cv2.getTextSize(studentInfo['name'], cv2.FONT_HERSHEY_COMPLEX, 1, 1)\n",
    "                offset = (414 - w) // 2\n",
    "                cv2.putText(imgBackground, str(studentInfo['name']), (808 + offset, 445),\n",
    "                            cv2.FONT_HERSHEY_COMPLEX, 1, (50, 50, 50), 1)\n",
    "        \n",
    "                imgBackground[175:175 + 216, 909:909 + 216] = imgStudent\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            if counter >= 20:\n",
    "                counter = 0\n",
    "                modeType = 0\n",
    "                studentInfo = []\n",
    "                imgStudent = []\n",
    "                imgBackground[44:44 + 633, 808:808 + 414] = imgModeList[modeType]\n",
    " \n",
    "    else:\n",
    "        modeType = 0\n",
    "        counter = 0\n",
    "        \n",
    "    cv2.imshow(\"face identification\", imgBackground)    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa0cacd-a9b5-4cf7-a58f-21d64eba92b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
